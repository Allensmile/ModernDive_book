# Confidence Intervals {#confidence-intervals}
    
```{r setup_ci, include=FALSE, purl=FALSE}
chap <- 9
lc <- 0
rq <- 0
# **`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**
# **`r paste0("(RQ", chap, ".", (rq <- rq + 1), ")")`**

knitr::opts_chunk$set(
  tidy = FALSE, 
  out.width = '\\textwidth', 
  fig.height = 4,
  warning = FALSE
  )

options(scipen = 99)#, digits = 3)
options(pillar.sigfig = 6)

# Set random number generator seed value for replicable pseudorandomness. Why 76?
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
```

In Chapter \@ref(sampling), we studied sampling. We started with a "tactile" exercise where we wanted to know the proportion of balls in the sampling bowl in Figure \@ref(fig:sampling-exercise-1) that are red. While we could have performed an exhaustive count, this would have been a tedious process. So instead we used a shovel to extract a sample of 50 balls and used the resulting proportion that were red as an estimate of the proportion of the bowl's balls that are red. Furthermore, we made sure to mix the bowl's contents before every use of the shovel. Because of the randomness induced by the mixing, different uses of the shovel yielded different proportions red and hence different estimates of the proportion of the bowl's balls that are red. 

We then mimicked this "tactile" exercise with an equivalent "virtual" exercise performed on the computer. Using our computers' random number generator, we could very quickly mimic the above sampling procedure a large number of times. In Section \@ref(different-shovels), we quickly repeated the above sampling procedure 1000 times using three different "virtual" shovels with 25, 50, and 100 slots. We compared the variation of these three sets of 1000 estimates of the proportion of the bowl's balls that are red in the three histograms in Figure \@ref(fig:comparing-sampling-distributions-3). 

What we did here was construct *sampling distributions*. The motivation for taking 1000 repeated samples and visualizing the resulting estimates was to study how these estimates varied from one sample to another; in other words we wanted to study the effect of *sampling variation*. We quantified the variation of these estimates using their standard deviation which has a special name: the *standard error*. In particular, we saw that as the sample size increased from 25 to 50 to 100, the standard error decreased and thus the sampling distributions narrowed. In other words, larger sample sizes lead to more *precise* estimates. 

We also described the above sampling exercises using the terminology and mathematical notation related to sampling we introduced in Section \@ref(terminology-and-notation). Our *study population* was the large bowl with $N$ = 2400 balls, while the *population parameter*, the unknown quantity of interest, here was the population proportion $p$ of the bowl's balls that are red. Since performing a *census* would be very expensive in terms of time and energy, we instead extracted a *sample* of size $n$ = 50. The *point estimate*, also known as a *sample statistic*, used to estimate $p$ was the sample proportion $\widehat{p}$ of these 50 sampled balls that were red. Furthermore, since the sample was obtained at *random*, it can be considered as *unbiased* and *representative* of the population. Thus any results based on the sample could be *generalized* to the population. In other words, the sample proportion $\widehat{p}$ of the shovel's $n$ = 50 balls that were red was a "good guess" of the true population proportion $p$ of the bowl's $N$ = 2400 balls that are red. In other words, we used the sample to *infer* about the population.

However as described in Section \@ref(sampling-simulation), both the tactile and virtual sampling exercises are not what one would do in real life; they were merely *simulations* used to study the effects of sampling variation. In a real life situation, we would not take 1000 samples of size $n$, but rather take a *single* representative sample of as large a size as possible. Additionally, we knew what the true value of the population parameter here was: the true population proportion of the bowl's balls that are red. In a real life situation we will not know what this value is. Because if we did, then why would we take a sample to estimate it? 

An example of a realistic sampling situation would be a poll, like the one described in the [Obama poll](https://www.npr.org/sections/itsallpolitics/2013/12/04/248793753/poll-support-for-obama-among-young-americans-eroding) article you saw in Section \@ref(sampling-case-study). Pollsters did not know the true proportion of \textif{all} young Americans who supported President Obama and thus took a single sample of size $n$ = 2089 young Americans to estimate the true unknown value.

So how does one study the effects of sampling variation when you only have a single sample to work with? There is no sample-to-sample variation is estimates when you only have one sample. One common method is known as *bootstrapping resampling*, which will be the focus of the earlier sections of this chapter. 

Furthermore, what if we would like not only a single estimate of the unknown population parameter, we would like a *range of highly plausible* values? Going back to the Obama poll article, it stated that the pollsters' estimate of the proportion of all young Americans who supported President Obama was 41%, but in addition it stated that the poll's "margin of error was plus or minus 2.1 percentage points." In other words this "plausible range" was [41% - 2.1%, 41% + 2.1%] = [37.9%, 43.1%]. This range of plausible values is known as a *confidence interval* and will be the focus of the later sections of this chapter. 


### Needed packages {-}

Let's load all the packages needed for this chapter (this assumes you've already installed them). Recall from our discussion in Section \@ref(tidyverse-package) that loading the `tidyverse` package by running `library(tidyverse)` loads the following commonly used data science packages all at once:

* `ggplot2` for data visualization
* `dplyr` for data wrangling
* `tidyr` for converting data to "tidy" format
* `readr` for importing spreadsheet data into R
* As well as the more advanced `purrr`, `tibble`, `stringr`, and `forcats` packages

If needed, read Section \@ref(packages) for information on how to install and load R packages. 

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(moderndive)
library(infer)
library(janitor)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Packages needed internally, but not in the text
library(knitr)
library(kableExtra)
library(patchwork)
# Ensure pennies_sample is loaded (might not be needed)
data("pennies_sample")
```

***



## Resampling activity

As we did in Chapter \@ref(sampling), we'll begin with a hands-on tactile activity.

### What is the average year on US pennies in 2019?

Try to imagine all pennies being used in the United States in 2019. That's a lot of pennies! Now say we're are interested in the average year of minting of *all* these pennies. One way to compute this value would be to gather up all pennies being used in the US, record the year, and compute the average. This would be near impossible! So instead, let's collect a sample of 50 pennies collected from a local bank in downtown Northampton, Massachusetts, USA seen in Figure \@ref(fig:resampling-exercise-a).

```{r resampling-exercise-a, echo=FALSE, fig.show='hold', fig.cap="Collecting a sample of 50 US pennies from a local bank.", purl=FALSE, out.width = "40%"}
knitr::include_graphics(c("images/sampling/pennies/bank.jpg", "images/sampling/pennies/roll.jpg"))
```

An image of these 50 pennies can be seen in Figure \@ref(fig:resampling-exercise-b).

```{r resampling-exercise-b, echo=FALSE, fig.cap="50 US pennies.", purl=FALSE, out.width = "100%"}
knitr::include_graphics("images/sampling/pennies/pennies_trim.jpg")
```

For each of the 50 pennies let's assign an "ID" label and mark the year of minting, starting in the top left and ending in the bottom right progressing row by row, as seen in Figure \@ref(fig:resampling-exercise-c).

```{r resampling-exercise-c, echo=FALSE, fig.cap="50 US pennies labelled.", fig.show='hold', purl=FALSE, out.width = "10%"}
labelled <- list.files("images/sampling/pennies/labelled/", full.names = TRUE)
knitr::include_graphics(labelled)
```

The `moderndive` package contains this data on our 50 sampled pennies. Let's explore this sample data first:

```{r}
pennies_sample
```

The `pennies_sample` data frame has 50 rows corresponding to each penny with two variables. The first variable `ID` correspond to the ID labels in Figure \@ref(fig:resampling-exercise-c) whereas the second variable `year` corresponds to the year of minting saved as an integer.

Based on these 50 sampled pennies, what can we say about *all* US pennies in 2019? Let's study some properties of our sample by performing an exploratory data analysis. Let's first visualize the distribution of the year of these 50 pennies using our data visualization tools from Chapter \@ref(viz). Since `year` is a numerical variable, we use a histogram in Figure \@ref(fig:pennies-sample-histogram).

```{r pennies-sample-histogram, fig.cap="Distribution of year on 50 US pennies."}
ggplot(pennies_sample, aes(x = year)) +
  geom_histogram(binwidth = 10, color = "white")
```

We observe a slightly left-skewed distribution since most values fall in between the 1980s through 2010s with only a few older than 1970. What is the average year for the 50 sampled pennies? Eyeballing the histogram it appears to be around 1990. Let's now compute this value exactly using our data wrangling tools from Chapter \@ref(wrangling).

```{r}
pennies_sample %>% 
  summarize(mean_year = mean(year))
```
```{r, echo=FALSE}
x_bar <- pennies_sample %>% 
  summarize(mean_year = mean(year))
```

Thus assuming `pennies_sample` is a representative sample from the population of all US pennies, a "good guess" of the average year of minting of all US pennies would be `r x_bar %>% pull(mean_year) %>% round(2)`, the average year of minting of our 50 sampled pennies. This should all start sounding similar to what we did previously in Chapter \@ref(sampling)!

In Chapter \@ref(sampling) our study population was the bowl of $N$ = 2400 balls. Our population parameter of interest was the population proportion of these balls that were red, denoted mathematically by $p$. In order to estimate $p$, we extracted a sample of 50 balls using the shovel and computed the relevant point estimate: the sample proportion of these 50 balls that were red, denoted mathematically by $\widehat{p}$.

Here our study population is $N$ = whatever number of pennies are being used in the US, a value which we don't know and probably never will. The population parameter of interest is now the *population mean* year of all these pennies, a value denoted mathematically by the greek letter $\mu$ pronounced "mu". In order to estimate $\mu$, we went to bank and obtained a sample of 50 pennies and computed the relevant point estimate: the *sample mean* year of these 50 pennies, denoted mathematically by $\overline{x}$ pronounced "x-bar". An alternative and more intuitive notation for the sample mean is $\widehat{\mu}$. However this is unfortunately not as commonly used, so in this text we'll always denote the sample mean as $\overline{x}$.

We summarize the correspondence between the sampling bowl exercise in Chapter \@ref(sampling) and our pennies exercise in Table \@ref(tab:summarytable-ch8-b), which are the first two rows of the previously seen Table \@ref(tab:summarytable-ch8) of the various sampling scenarios we'll cover in this text.

```{r summarytable-ch8-b, echo=FALSE, message=FALSE}
# The following Google Doc is published to CSV and loaded below using read_csv() below:
# https://docs.google.com/spreadsheets/d/1QkOpnBGqOXGyJjwqx1T2O5G5D72wWGfWlPyufOgtkk4/edit#gid=0

"https://docs.google.com/spreadsheets/d/e/2PACX-1vRd6bBgNwM3z-AJ7o4gZOiPAdPfbTp_V15HVHRmOH5Fc9w62yaG-fEKtjNUD2wOSa5IJkrDMaEBjRnA/pub?gid=0&single=true&output=csv" %>% 
  read_csv(na = "") %>% 
  # Only first two scenarios
  filter(Scenario <= 2) %>% 
  kable(
    caption = "\\label{tab:summarytable-ch8}Scenarios of sampling for inference", 
    booktabs = TRUE,
    escape = FALSE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position")) %>%
  column_spec(1, width = "0.5in") %>% 
  column_spec(2, width = "0.7in") %>%
  column_spec(3, width = "1in") %>%
  column_spec(4, width = "1.1in") %>% 
  column_spec(5, width = "1in")
```

Going back to our 50 sampled pennies in Figure \@ref(fig:resampling-exercise-c), the point estimate of interest is the sample mean $\overline{x}$ of `r x_bar %>% pull(mean_year) %>% round(2)`. This quantity is an estimate of the population mean year of all US pennies $\mu$.

Recall that we also saw in Chapter \@ref(sampling) that such estimates are prone to sampling variation. For example, in this particular sample in Figure \@ref(fig:resampling-exercise-c), we observed three pennies with a year of 1999. If we obtained other samples of size 50 would we always observe exactly three pennies with a year of 1999? More than likely not. We might observe none, or one, or two, or maybe even all 50! The same can be said for the other 26 unique years that are represented in our sample of 50 pennies.

To study this sampling variation as we did in Chapter \@ref(sampling), we need more than one sample. In our case with pennies, how would we obtain another sample? We would go to the bank and get another roll of 50 pennies! However, in real-life sampling one doesn't obtain many samples as we did in Chapter \@ref(sampling); those were merely simulations. So what how can we study sample-to-sample variation when we have only a single sample as in our case?

Just as different uses of the shovel in the bowl led to different sample proportions red, different samples of 50 pennies will lead to different sample mean years. However, how can we study the effect of sampling variation using only our *single sample* seen in Figure \@ref(fig:resampling-exercise-c)? We will do so using a technique known as "bootstrap resampling with replacement", which we now illustrate.



### Resampling once

**Step 1**: Let's print out identically-sized slips of paper representing the 50 pennies in Figure \@ref(fig:resampling-exercise-c).

```{r tactile-resampling-1, echo=FALSE, fig.cap="50 slips of paper representing 50 US pennies.", fig.show='hold', purl=FALSE, out.width = "50%"}
knitr::include_graphics("images/sampling/pennies/tactile_simulation/1_paper_slips.png")
```

**Step 2**: Put the 50 small pieces of paper into a hat or tuque.

```{r tactile-resampling-2, echo=FALSE, fig.cap="Putting 50 slips of paper in a hat.", fig.show='hold', purl=FALSE, out.width = "50%"}
knitr::include_graphics("images/sampling/pennies/tactile_simulation/2_insert_in_hat.png")
```

**Step 3**: Mix the hat's contents and draw one slip of paper at random. Record the year somewhere.

```{r tactile-resampling-3, echo=FALSE, fig.cap="Drawing one slip of paper.", fig.show='hold', purl=FALSE, out.width = "50%"}
knitr::include_graphics("images/sampling/pennies/tactile_simulation/3_draw_at_random.png")
```

**Step 4**: Put the slip of paper back in the hat! In other words, replace it! 

```{r tactile-resampling-4, echo=FALSE, fig.cap="Replacing slip of paper.", fig.show='hold', purl=FALSE, out.width = "50%"}
knitr::include_graphics("images/sampling/pennies/tactile_simulation/4_put_it_back.png")
```

**Step 5**: Repeat Steps 3 and 4 49 more times, resulting in 50 recorded years.

What we just performed was a **resampling** of the original sample of 50 pennies. We are not sampling 50 pennies from the population of all US pennies as we did in our trip to the bank. Instead we are mimicking this act by "re"-sampling 50 pennies from our originally sampled 50 pennies. However why did we replace our resampled slip of paper back into the hat in Step 4? Because if we left the slip of paper out of the hat each time we performed Step 4, we would obtain the same 50 pennies in the end each time! In other words, replacing the slips of paper induces variation.

Being more precise with our terminology, we just performed a **resampling with replacement** of the original sample of 50 pennies. Had we left the slip of paper out of the hat each time we performed Step 4, this would be "resampling without replacement".

Let's study our 50 resampled pennies via an exploratory data analysis. First, let's load the data into R by manually creating a data frame `pennies_resample` of our 50 resampled values. We'll do this using the `tibble()` command from the `dplyr` package. Note that the 50 values you obtained will almost certainly not be the same as ours.

```{r}
pennies_resample <- tibble(
  year = c(1976, 1962, 1976, 1983, 2017, 2015, 2015, 1962, 2016, 1976, 
           2006, 1997, 1988, 2015, 2015, 1988, 2016, 1978, 1979, 1997, 
           1974, 2013, 1978, 2015, 2008, 1982, 1986, 1979, 1981, 2004, 
           2000, 1995, 1999, 2006, 1979, 2015, 1979, 1998, 1981, 2015, 
           2000, 1999, 1988, 2017, 1992, 1997, 1990, 1988, 2006, 2000)
)
```

The 50 values of `year` in `pennies_resample` represent the resample of size 50 from the original sample of 50 pennies from the bank. We display the 50 resampled pennies in Figure \@ref(fig:resampling-exercise-d).

```{r resampling-exercise-d, echo=FALSE, fig.cap="50 resampled US pennies labelled", fig.show='hold', purl=FALSE, out.width="10%"}
# Need this for ID column
if(!file.exists("rds/pennies_resample.rds")){
  pennies_resample <- pennies_sample %>% 
    rep_sample_n(size = 50, replace = TRUE, reps = 1) %>% 
    ungroup() %>% 
    select(-replicate)
  write_rds(pennies_resample, "rds/pennies_resample.rds")
} else {
  pennies_resample <- read_rds("rds/pennies_resample.rds")
}

# Should probably switch to here::here() in this example
resampled_labelled <- paste0(getwd(),
                             "/images/sampling/pennies/labelled/",
                             str_pad(pennies_resample$ID, 2, pad = "0"), 
                             ".jpg")
knitr::include_graphics(resampled_labelled)
```

Let's compare the distribution of the numerical variable `year` of our 50 resampled pennies with the distribution of the numerical variable `year` of our original sample of 50 pennies from the bank in Figure \@ref(orig-and-resample).

```{r eval=FALSE}
ggplot(pennies_resample, aes(x = year)) +
  geom_histogram(binwidth = 10, color = "white") +
  labs(title = "Resample of 50 pennies")
ggplot(pennies_sample, aes(x = year)) +
  geom_histogram(binwidth = 10, color = "white") +
  labs(title = "Original sample of 50 pennies")
```

(ref:compare-plots) Comparing `year` in the resample `pennies_resample` with the original sample `pennies_sample`.

```{r orig-and-resample, echo=FALSE, fig.cap="(ref:compare-plots)", purl=FALSE}
p1 <- 
  ggplot(pennies_resample, aes(x = year)) +
  geom_histogram(binwidth = 10, color = "white") +
  labs(title = "Resample of 50 pennies") +
  coord_cartesian(xlim = seq(1950, 2030, 20), ylim = seq(0, 15, 5))
p2 <- 
  ggplot(pennies_sample, aes(x = year)) +
  geom_histogram(binwidth = 10, color = "white") +
  labs(title = "Original sample of 50 pennies") +
  coord_cartesian(xlim = seq(1950, 2030, 20), ylim = seq(0, 15, 5))
p1 + p2
```

Observe that while the general shape of the distribution of `year` is roughly similar, they are not identical. This is due to the variation induced by replacing the slips of paper each time we pull one out and recorded the year. 

Recall from the previous section that the sample mean of the original sample of 50 pennies from the bank was `r x_bar %>% pull(mean_year) %>% round(2)`. What about for the `year` variable in `pennies_resample`? Any guesses? Let's have `dplyr` help us out as before:

```{r}
pennies_resample %>% 
  summarize(mean_year = mean(year))
```
```{r, echo=FALSE}
resample_mean <- pennies_resample %>% 
  summarize(mean_year = mean(year))
```

We obtained a different mean year of `r resample_mean %>% pull(mean_year) %>% round(2)`. Again, this variation is induced by the "with replacement" from the "resampling with replacement" terminology we defined earlier. 

What if we repeated several times this resampling exercise many times? Would we obtain the same sample mean `year` value each time? In other words, would our guess at the mean year of all pennies in the US in 2019 be exactly `r resample_mean %>% pull(mean_year) %>% round(2)` every time? Just as we did in Chapter \@ref(sampling), let's perform this resampling activity with the help of 35 of our friends.


### Resampling 35 times {#student-resamples}

Each of our 35 friends will repeat the same 5 steps above:

1. Start with 50 identically-sized slips of paper representing the 50 pennies. 
1. Put the 50 small pieces of paper into a hat or tuque.
1. Mix the hat's contents and draw one slip of paper at random. Record the year somewhere.
1. Replace the slip of paper back in the hat! 
1. Repeat Steps 3 and 4 49 more times, resulting in 50 recorded years.

Since we had 35 of our friends perform this task, we ended up with 35 $\times$ 50 = 1750 values. We recorded these values in a [shared spreadsheet](https://docs.google.com/spreadsheets/d/1y3kOsU_wDrDd5eiJbEtLeHT9L5SvpZb_TrzwFBsouk0/) with 50 rows (plus a header row) and 35 columns; we display a snapshot of the first 10 rows and 5 columns in Figure \@ref(fig:tactile-resampling-5)

```{r tactile-resampling-5, echo=FALSE, fig.cap = "Snapshot of shared spreadsheet of resampled pennies.", fig.show='hold', purl=FALSE, out.width = "50%"}
knitr::include_graphics("images/sampling/pennies/tactile_simulation/5_shared_spreadsheet.png")
```

For your convenience, we've taken these 35 $\times$ 50 = 1750 values and saved them in `virtual_resamples`, a "tidy" data frame included in the `moderndive` package: 

```{r}
pennies_resamples
```

What did each of our 35 friends obtain as the mean year? `dplyr` to the rescue once more! After grouping the rows by `name`, we summarize each group of rows with their mean `year`:

```{r}
resampled_means <- pennies_resamples %>% 
  group_by(name) %>% 
  summarize(mean_year = mean(year))
resampled_means
```

Observe that `resampled_means` has 35 rows corresponding to the 35 resample means based the 35 resamples performed by our friends. Furthermore, observe the variation in the 35 values in the variable `mean_year`. This variation is exists because by "resampling with replacement", our 35 friends obtained different resamples of 50 pennies, and thus obtained different resample mean year. 

Since the variable `mean_year` is numerical, let's visualize its distribution using a histogram in Figure \@ref(fig:tactile-resampling-6). Note that adding the argument `boundary = 1990` to `geom_histogram()` sets the binning structure of the histogram so that one of the boundaries between bins is at the year 1990 exactly. 

```{r tactile-resampling-6, echo=TRUE, fig.cap="Distribution of 35 sample means from 35 resamples.", purl=FALSE}
ggplot(resampled_means, aes(x = mean_year)) +
  geom_histogram(binwidth = 1, color = "white", boundary = 1990) +
  labs(x = "Resampled mean year")
```

Observe the following about the histogram in Figure \@ref(fig:tactile-resampling-6):

* The distribution looks roughly normal.
* We rarely observe sample mean years less than 1992.
* On the other side of the distribution, we rarely observe sample mean years greater than 2000.
* The most frequently occurring values occur between roughly 1992 and 1998.
* The distribution of these 35 sample means based on 35 resamples is roughly centered at 1995, which is the sample mean of `r x_bar %>% pull(mean_year) %>% round(2)` of the original sample of 50 pennies from the bank.


### What's the plan?

What we just demonstrated in this activity is the statistical procedure known as *bootstrap resampling with replacement*. We used *resampling* to mimic the sampling variation we observe from sample-to-sample as we did in Chapter Chapter \@ref(sampling) on sampling, but this time using a *single* sample from the population.

In fact, the histogram of sample means from 35 resamples in Figure \@ref(fig:tactile-resampling-6) is called the *bootstrap distribution* of the sample mean and it is an approximation of the *sampling distribution* of the same mean, a concept we introduced in Chapter \@ref(sampling). Using this bootstrap distribution, we can study the effect of sampling variation on our estimates, in particular study the typical "error" of our estimates, known as the *standard error*. 

In Section \@ref(resampling-simulation) we'll mimic our tactile resampling activity virtually on the computer. We can use a computer to do the resampling many more times than our 35 friends could possibly do. This will allow us to better understand the bootstrap distribution. In Section \@ref(ci-build-up) we'll explicitly articulate our goals for this chapter: understanding resampling variation, defining the statistical concept of a *confidence interval* by building on our pennies example, and discussing the interpretation of confidence intervals.

Following this framework on confidence intervals, we'll discuss the `dplyr` and `infer` package code needed to complete the process of *bootstrapping*, which is another name for this resampling approach that is most commonly found in developing confidence intervals. We've used one of the functions in the `infer` package already with `rep_sample_n()`, but there's a lot more to this package than just that. We'll introduce the tidy statistical inference framework that was the motivation for the `infer` package pipeline that will be the driving package throughout the rest of this book.

As we did in Chapter \@ref(sampling), we'll tie all these ideas together with a real-life case study in Section \@ref(case-study-two-prop-ci) involving data from an experiment about yawning from the US television show Mythbusters. The chapter concludes with a comparison of the sampling distribution and a bootstrap distribution using the balls data from Chapter \@ref(sampling) on sampling. 



***



## Computer simulation of resampling {#resampling-simulation}

Let's now mimic our tactile resampling activity virtually by using our computer.

### Virtually resampling once

First, let's perform the virtual analog of resampling once. Recall that the `pennies_sample` data frame included in the `moderndive` package contains the years of our original sample of 50 pennies from the bank. Furthermore recall in Chapter \@ref(sampling) on sampling that we used the `rep_sample_n()` function as a virtual shovel to sample balls from our virtual bowl of `r nrow(bowl)` balls. 

```{r, eval=FALSE, purl=FALSE}
virtual_shovel <- bowl %>% 
  rep_sample_n(size = 50)
```

Let's combine these two elements to virtually mimic our resampling with replacement exercise involving the slips of paper representing our 50 pennies in Figure \@ref(fig:resampling-exercise-c):

```{r eval=FALSE}
virtual_resample <- pennies_sample %>% 
  rep_sample_n(size = 50, replace = TRUE)
```

Observe how we explicitly set the `replace` argument to `TRUE` in order to tell `rep_sample_n()` that we would like to sample pennies *with* replacement. Had we not set `replace = TRUE`, the function would've assumed the default value of `FALSE`. Additionally, since we didn't specify the number of replicates via the `reps` argument, the function assumes the default of one replicate `reps = 1`. Note also that the `size` argument is set to match the original sample size of 50 pennies. So what does `virtual_resample` look like?

```{r eval=FALSE}
View(virtual_resample)
```

We'll display only the first 10 out of 50 rows of `virtual_resample`'s contents in Table \@ref(tab:virtual-shovel).

```{r virtual-resample, echo=FALSE}
virtual_resample <- pennies_sample %>% 
  rep_sample_n(size = 50, replace = TRUE, reps = 1)
virtual_resample %>% 
  slice(1:10) %>%
  knitr::kable(
    align = c("r", "r", "r"),
    digits = 3,
    caption = "First 10 resampled rows of 50 in virtual sample",
    booktabs = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position"))
```

The `replicate` variable only takes on the value of 1 corresponding to us only having `reps = 1`, the `ID` variable indexes which of the 50 pennies from `pennies_sample` was resampled, and `year` denotes the year of minting. 

Let's compute the mean `year` in our virtual resample of size 50 using data wrangling functions included in the `dplyr` package:

```{r}
virtual_resample %>% 
  summarize(resample_mean = mean(year))
```

As when we did our tactile resampling, the resulting mean year is different than that mean year of our 50 originally sampled pennies of `r x_bar %>% pull(mean_year) %>% round(2)`.

<!-- 
Chester: Not sure if needed, but those trying to follow along may be mystified if we don't include this. 

Note that tibbles will try to print as pretty as possible which may result in numbers being rounded. In this chapter, we have set the default number of values to be printed to six in tibbles with `options(pillar.sigfig = 6)`.

Albert: I'm not sure if it's worth trouble to explain that command and why tidyverse opts for 3 sigfigs.
-->

### Virtually resampling 35 times

Let's now have 35 virtual friends perform our virtual resampling exercise. Using these results, we'll be able to study the variability in the sample means from 35 resamples of size 50. Let's first add a `reps = 35` argument to `rep_sample_n()` to indicate we would like 35 replicates, or in other words repeat the resampling with replacement of 50 pennies 35 times.

```{r}
virtual_resamples <- pennies_sample %>% 
  rep_sample_n(size = 50, replace = TRUE, reps = 35)
virtual_resamples
```

The resulting `virtual_resamples` data frame has 35 $\times$ 50 = `r 35*50` rows corresponding to 35 resamples of 50 pennies. What did each of our 35 virtual friends obtain as the mean year? We'll use the same `dplyr` verbs as we did in the previous section, but computing the mean for each of our virtual friends separately by adding a `group_by(replicate)`:

```{r, eval=TRUE}
virtual_resampled_means <- virtual_resamples %>% 
  group_by(replicate) %>% 
  summarize(mean_year = mean(year))
virtual_resampled_means
```

Observe that `virtual_resampled_means` has 35 rows corresponding to the 35 resampled means and that the values of `mean_year` vary. Let's visualize the distribution of the numerical variable `mean_year` using a histogram in Figure \@ref(fig:tactile-resampling-7).

```{r tactile-resampling-7, echo=TRUE, fig.cap="Distribution of 35 sample means from 35 resamples.", purl=FALSE}
ggplot(virtual_resampled_means, aes(x = mean_year)) +
  geom_histogram(binwidth = 1, color = "white", boundary = 1990) +
  labs(x = "Resampled mean year")
```

To convince ourselves that our virtual resampling indeed mimics the resampling done by our 35 friends, let's compare the bootstrap distribution we just virtually constructed with the bootstrap distribution our 35 friends constructed via tactile resampling in the previous section.

```{r orig-and-resample-means, echo=FALSE, fig.cap="Comparing distributions of means from resamples`", purl=FALSE}
p3 <- ggplot(virtual_resampled_means, aes(x = mean_year)) +
  geom_histogram(binwidth = 1, color = "white", boundary = 1990) +
  labs(x = "Resampled mean year", title = "35 means of tactile resamples") +
  scale_x_continuous(breaks = seq(1990, 2000, 2))
p4 <- ggplot(resampled_means, aes(x = mean_year)) +
  geom_histogram(binwidth = 1, color = "white", boundary = 1990) +
  labs(x = "Resampled mean year", title = "35 means of virtual resamples") +
  scale_x_continuous(breaks = seq(1990, 2000, 2))
p3 + p4
```

Recall that in the "resampling with replacement" scenario we are illustrating here both the above histograms have a special name: the *bootstrap distribution of the sample mean*. Furthermore, they are an approximation to the *sampling distribution* of the sample mean, a concept you saw in Chapter \@ref(sampling) on sampling. These distributions allow us to study the effect of sampling variation on our estimates of the true population mean, in this case the true mean year for *all* US pennies. However, unlike in Chapter \@ref(sampling) where we simulated the act of taking multiple samples, something one would never do in practice, bootstrap distributions are constructed from a *single* sample, in this case the 50 original pennies from the bank. 

```{block, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Ask learners to compare the distributions since we did something similar in Chapter 8 and they should be well versed on this by now.

```{block, type='learncheck', purl=FALSE}
```


### Virtually resampling 1000 times {#bootstrap-1000-replicates}

Remember that one of the goals of resampling with replacement is to construct the bootstrap distribution, which is an approximation of the sampling distribution of the point estimate of interest, here the sample mean year. However the bootstrap distribution of in Figure \@ref(fig:tactile-resampling-7) is based only on 35 resamples and hence looks a little coarse. Let's increase the number of resamples to 1000 to better observe the shape and the variability from one resample to the next. 

```{r}
# Repeat resampling 1000 times
virtual_resamples <- pennies_sample %>% 
  rep_sample_n(size = 50, replace = TRUE, reps = 1000)

# Compute 1000 sample means
virtual_resampled_means <- virtual_resamples %>% 
  group_by(replicate) %>% 
  summarize(mean_year = mean(year))
```

However, in the interest of brevity, going forward let's combine the above two operations into a single chain of `%>%` pipe operators:

```{r}
virtual_resampled_means <- pennies_sample %>% 
  rep_sample_n(size = 50, replace = TRUE, reps = 1000) %>% 
  group_by(replicate) %>% 
  summarize(mean_year = mean(year))
virtual_resampled_means
```

Let's visualize the bootstrap distribution of these 1000 sample means from 1000 virtual resamples looks like in Figure \@ref(fig:one-thousand-sample-means):

```{r one-thousand-sample-means, echo=FALSE, message=FALSE, fig.cap="Bootstrap resampling distribution based on 1000 resamples."}
ggplot(virtual_resampled_means, aes(x = mean_year)) +
  geom_histogram(binwidth = 1, color = "white", boundary = 1990) +
  labs(x = "sample mean")
```

Note here the bell shape starting to become more apparent. We now have a general sense for the range of values that the sample mean may take on in these resamples from this histogram of the bootstrap distribution. Do you have a guess as to where this histogram is centered? With it being close to symmetric, either the mean or the median would serve as a good estimate for the center here. Let's compute the mean:

```{r eval=TRUE}
virtual_resampled_means %>% 
  summarize(mean_of_means = mean(mean_year))
```
```{r echo=FALSE}
mean_of_means <- virtual_resampled_means %>% 
  summarize(mean(mean_year)) %>% 
  pull()
```

The mean of the 1000 means from 1000 resamples is `r mean_of_means`. Note that this is quite close to the mean of our original sample of 50 pennies from the bank: `r x_bar %>% pull(mean_year) %>% round(2)`. This is the case since each of the 1000 resamples are based on the original sample of 50 pennies.



```{block, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What is the difference between a bootstrap distribution and a sampling distribution?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Ask learners to summarize important features of the plot as was done in Chapter 8.

```{block, type='learncheck', purl=FALSE}
```



***



## Understanding confidence intervals {#ci-build-up}

Let's start this section with an analogy involving fishing. Say you are trying to catch a fish. On the one hand you could use a spear, while on the other you could use a net. Using the net will probably yield better results! Bringing things back to the pennies: you are trying to estimate the true population mean year $\mu$ of all US pennies. Think of the value of $\mu$ as the fish.

On the one hand, we could use the appropriate point estimate/sample statistic to estimate $\mu$, which we saw in Table \@ref(tab:summarytable-ch8-b) is the sample mean $\overline{x}$. Based on our sample of 50 pennies from the bank, the sample mean was `r x_bar %>% pull(mean_year) %>% round(2)`. Think of this value as fishing with a spear. 

On the other hand, let's use our results from the previous section to construct a range of highly probable values for $\mu$. Looking at the bootstrap distribution in Figure \@ref(fig:one-thousand-sample-means), between which two years would you say that "most" sample means lie?  While this question is somewhat subjective, saying that most sample means lie in the interval 1992 to 2000 would not be unreasonable. Think of this interval as fishing with a net.

What we've just illustrated is the concept of a *confidence interval*, which we'll abbreviate with "CI" throughout this book. So as opposed to a point estimate/sample statistic that estimates the value of an unknown population parameter with a single value, a *confidence interval* gives a range of plausible values. Going back to our analogy, point estimates/sample statistics can be thought of as spears, whereas confidence intervals can be thought of as nets. 

Point estimate/sample statistic           |  Confidence interval
:-------------------------:|:-------------------------:
![](images/spear.jpg){ height=1.7in }  |  ![](images/net2.jpg){ height=1.7in }

Our proposed interval of highly probably values for $\mu$ of 1992 to 2000 was constructed by eye and is thus somewhat subjective. We now introduce two methods for constructing such intervals in a more principled fashion: the percentile method and the standard error method.

Both methods for confidence interval construction share some commonalities. First, they are both constructed from the bootstrap distribution, an example of which you created using 1000 bootstrap resamples with replacement in Subsection \@ref(bootstrap-1000-replicates) and saved in the `virtual_resampled_means` data frame. 

Second, they both require you to specify the *confidence level*.  All other things being equal, higher confidence levels correspond to wider confidence intervals and lower confidence levels corresponding to narrower confidence intervals. Commonly used confidence levels include 90%, 95%, and 99%; we'll be mostly using 95% and hence constructing "95% confidence intervals for $\mu$".


### The percentile method {#percentile-method}

```{r echo=FALSE}
# Can also use conf_int() and get_confidence_interval() instead of get_ci(),
# as they are aliases that work the exact same way.
percentile_ci <- virtual_resampled_means %>% 
  rename(stat = mean_year) %>% 
  get_ci(level = 0.95, type = "percentile")
```

Recall that the actual population mean year $\mu$ for all pennies in circulation in the US is unknown to us. The only way to know this value exactly would be to conduct a census of all pennies, a near impossible task. Instead, by constructing a confidence interval we'll obtain a range of plausible values for this unknown $\mu$. 

One method to construct this range is to use the middle 95% of the 1000 sample means we computed using bootstrap resampling with replacement. We can do this by computing the 2.5^th^ and 97.5^th^ percentiles, which are `r percentile_ci[["2.5%"]]` and `r percentile_ci[["97.5%"]]` respectively. For now, let's focus on the concepts behind a percentile method constructed confidence interval; we'll show you the code to compute these values in the next section.

We can mark these percentiles on the bootstrap distribution with red vertical lines in Figure \@ref(fig:percentile-method). You can see that 95% of the values in the `mean_year` variable in `virtual_resampled_means` fall between the two endpoints, with 2.5% to the left of the left-most red line and 2.5% to the right of the right-most red line. 

```{r percentile-method, echo=FALSE, message=FALSE, fig.cap="Percentile method 95% confidence interval."}
ggplot(virtual_resampled_means, aes(x = mean_year)) +
  geom_histogram(binwidth = 1, color = "white", boundary = 1988) +
  labs(x = "sample mean") +
  scale_x_continuous(breaks = seq(1988, 2006, 2)) +
  geom_vline(xintercept = percentile_ci[[1, 1]], color = "red", size = 1) +
  geom_vline(xintercept = percentile_ci[[1, 2]], color = "red", size = 1)
```


### The standard error method

```{r echo=FALSE}
# Can also use conf_int() and get_confidence_interval() instead of get_ci(),
# as they are aliases that work the exact same way.
standard_error_ci <- virtual_resampled_means %>% 
  rename(stat = mean_year) %>% 
  get_ci(type = "se", point_estimate = x_bar)

# bootstrap SE value as scalar
bootstrap_se <- virtual_resampled_means %>% 
  summarize(se = sd(mean_year)) %>% 
  pull(se)
```

Recall in Subsection \@ref(normal-distribution), we saw that if a numerical variable follows a normal distribution, or in other words the histogram of this variable is bell shaped, then roughly 95% of values fall between $\pm$ 1.96 standard deviations of the mean. Given that our bootstrap distribution based on 1000 resamples with replacement in Figure \@ref(fig:one-thousand-sample-means) is normally shaped, let's use the above fact about normal distributions to construct a confidence interval in a different way.

First, the bootstrap distribution has mean equal to $\overline{x}$: the sample mean of our original 50 pennies of `r x_bar %>% pull(mean_year) %>% round(2)`. In other words, the bootstrap distribution is centered at `r x_bar %>% pull(mean_year) %>% round(2)`. Second, let's compute the standard deviation of the bootstrap distribution

```{r}
virtual_resampled_means %>% 
  summarize(SE = sd(mean_year))
```

What is this value? Recall that the bootstrap distribution is an approximation to the sampling distribution and that the standard deviation of the sampling distribution has a special name: the *standard error*. So in other words, `r bootstrap_se %>% round(2)` is an approximation of the standard error of $\overline{x}$.  

Thus using our 95% rule about normal distributions, we can use the following formula to determine the lower and upper endpoints of the 95% confidence interval for $\mu$:

$$
\begin{align}
\overline{x} \pm 1.96 \cdot SE &= (\overline{x} - 1.96 \cdot SE, \overline{x} + 1.96 \cdot SE)\\
&= (`r x_bar %>% pull(mean_year) %>% round(2)` - 1.96 \cdot `r bootstrap_se %>% round(2)`, `r x_bar %>% pull(mean_year) %>% round(2)` + 1.96 \cdot `r bootstrap_se %>% round(2)`)\\
&= (1991.15, 1999.73)
\end{align}
$$

Let's add the SE method confidence interval (in blue) to our previously constructed percentile method confidence (in red) in Figure \@ref(fig:percentile-and-se-method).

```{r percentile-and-se-method, echo=FALSE, message=FALSE, fig.cap="Comparing 95% confidence interval methods."}
both_CI <- bind_rows(
  percentile_ci %>% gather(endpoint, value) %>% mutate(type = "percentile"),
  standard_error_ci %>% gather(endpoint, value) %>% mutate(type = "SE")
)
ggplot(virtual_resampled_means, aes(x = mean_year)) +
  geom_histogram(binwidth = 1, color = "white", boundary = 1988) +
  labs(x = "sample mean", title = "Percentile method CI in red, SE method CI in blue") +
  scale_x_continuous(breaks = seq(1988, 2006, 2)) +
  geom_vline(xintercept = percentile_ci[[1, 1]], color = "red", size = 1) +
  geom_vline(xintercept = percentile_ci[[1, 2]], color = "red", size = 1) + 
  geom_vline(xintercept = standard_error_ci[[1, 1]], color = "blue", size = 1) +
  geom_vline(xintercept = standard_error_ci[[1, 2]], color = "blue", size = 1)
```

We see that both methods produce nearly identical confidence intervals with the percentile method yielding $(`r round(percentile_ci[["2.5%"]], 2)`, `r round(percentile_ci[["97.5%"]], 2)`)$ while the standard error method being $(`r round(standard_error_ci[["lower"]], 2)`, `r round(standard_error_ci[["upper"]],2)`)$. However, recall that we can only use the standard error rule when the bootstrap distribution is roughly normally-shaped. 

Now that we've introduced the concept of confidence intervals and laid out the intuition behind two methods for constructing them, let's explore the code that allows us to construct them. 

<!--
The variability of the sampling distribution may be approximated by the variability of the resampling distribution. Traditional theory-based methodologies for inference also have formulas for standard errors, assuming some conditions are met.

This is done by using the formula where $\bar{x}$ is our original sample mean and $SE$ stands for **standard error** and corresponds to the standard deviation of the resampling distribution.  The value of $multiplier$ here is the appropriate percentile of the standard normal distribution. We'll go into this further in Section \@ref(ci-conclusion).

These are automatically calculated when `level` is provided with `level = 0.95` being the default. (95% of the values in a standard normal distribution fall within 1.96 standard deviations of the mean, so $multiplier = 1.96$ for `level = 0.95`, for example.)  As mentioned, this formula assumes that the bootstrap distribution is symmetric and bell-shaped. This is often the case with bootstrap distributions, especially those in which the original distribution of the sample is not highly skewed.

This $\bar{x} \pm (multiplier * SE)$ formula is implemented in the `get_ci()` function as shown with our pennies problem using the bootstrap distribution's variability as an approximation for the sampling distribution's variability. We'll see more on this approximation shortly.

Note that the center of the confidence interval (the `point_estimate`) must be provided for the standard error confidence interval.

```{r eval=FALSE}
standard_error_ci <- bootstrap_distribution %>% 
  get_ci(type = "se", point_estimate = x_bar)
standard_error_ci
```
-->


```{block, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** What condition about the bootstrap distribution must be met for us to be able to construct confidence intervals using the standard error method?

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Say we wanted to contruct a 68% confidence interval instead of a 95% confidence interval for $\mu$?

```{block, type='learncheck', purl=FALSE}
```



***



## Constructing confidence intervals {#bootstrap-process}

The way in which we used resampling with replacement to get a range of plausible values for an unknown parameter is known as **bootstrapping.** To better understand this term, we harken back to the idea of "pulling oneself up by their bootstraps." To "pull oneself up by their bootstraps" means to ["succeed only by one's own efforts or abilities."](https://en.wiktionary.org/wiki/pull_oneself_up_by_one%27s_bootstraps) From a statistical perspective, we have pulled ourselves up from our bootstraps by using a single sample: the 50 pennies from the bank in the `pennies_sample` data frame. Thus, we've used only the "effort" of the single original sample towards the larger goal of approximating the variability of the sampling distribution.

Bootstrapping uses a process of sampling with replacement from our original sample to create new **bootstrap samples** of the *same* size as our original sample. We can again make use of the `rep_sample_n()` function to explore what one such bootstrap sample would look like. Remember that we are randomly sampling from the original sample here with replacement and that we always use the same sample size for the bootstrap samples as the size of the original sample.

### The original workflow needed for this

In Section \@ref(resampling-simulation) how to develop these bootstrap samples, bootstrap statistics, and the resulting *bootstrap distribution*. We called this *bootstrap distribution* the *resampling distribution* before to drill home the idea of this being based on resampling (sampling with replacement) instead of sampling without replacement as seen in Chapter \@ref(sampling). Let's revisit the flow using the `%>%` but this time call it `bootstrap_distribution`.

First, we repeatedly (`reps = 1000`) sampled with replacement (`replace = TRUE`) from the original sample with the same size of the resample (`size = 50`) using the `rep_sample_n()` function in the `infer` package:

```{r eval=FALSE}
pennies_sample %>% 
  rep_sample_n(size = 50, replace = TRUE, reps = 1000)
```

Next, since we are looking to get the mean value for year across each `replicate`, we used the `dplyr` verb `group_by()` to set up that grouping for our next step:

```{r eval=FALSE}
 pennies_sample %>% 
  rep_sample_n(size = 50, replace = TRUE, reps = 1000) %>% 
  group_by(replicate) 
```

Lastly, we finish off the grouping by using `summarize()` from `dplyr` to get our mean value for year from each `replicate`. We also name this resulting 1000 row data frame `bootstrap_distribution`:

```{r eval=FALSE}
 bootstrap_distribution <- pennies_sample %>% 
  rep_sample_n(size = 50, replace = TRUE, reps = 1000) %>% 
  group_by(replicate) %>% 
  summarize(mean_year = mean(year))
```

```{r echo=FALSE}
bootstrap_distribution <- virtual_resampled_means
```

So for the case where we are bootstrapping a single variable like `year` that we have seen so far, we can get by with using the `rep_sample_n()` function and a couple `dplyr` verbs to get the bootstrap distribution. For more complicated inferential techniques we need a little more firepower though. Let's check out the `infer` package for tidy statistical inference!

### The infer package for statistical inference

The `infer` package makes great use of the `%>%` to create a pipeline for statistical inference. The goal of the package is to provide a way for its users to explain the computational process of confidence intervals and hypothesis tests using the code as a guide. The verbs build in order here, so you'll want to start with `specify()` and then continue through the others as needed.

#### Specify variables {-}

```{r fig.align='center', echo=FALSE, out.width='30%'}
knitr::include_graphics("images/flowcharts/infer/specify.png")
```

The `specify()` function is used primarily to choose which variables will be the focus of the statistical inference. In addition, a setting of which variable will act as the `explanatory` and which acts as the `response` variable is done here. For proportion problems similar to those in Chapter \@ref(sampling), we can also give which of the different levels we would like to have as a `success`. We'll see further examples of these options in this chapter, Chapter \@ref(hypothesis-testing), and in Appendix \@ref(appendixB).

To begin to create a confidence interval for the population mean year of US pennies in 2019, we start by using `specify()` to choose which variable in our `pennies_sample` data we'd like to work with. This can be done in one of two ways:

1. Using the `response` argument:

```{r}
pennies_sample %>% 
  specify(response = year)
```

2. Using `formula` notation:

```{r}
pennies_sample %>% 
  specify(formula = year ~ NULL)
```

Note that the formula notation uses the common R methodology to include the response $y$ variable on the left of the `~` and the explanatory $x$ variable on the right of the "tilde." Recall that you used this notation frequently with the `lm()` function in Chapters \@ref(regression) and \@ref(multiple-regression) when fitting regression models. Either notation works just fine, but preference is usually given here for the `formula` notation to further build on the ideas from earlier chapters.

#### Generate replicates {-}

```{r fig.align='center', echo=FALSE, out.width='70%'}
knitr::include_graphics("images/flowcharts/infer/generate.png")
```

After `specify()`ing the variables we'd like in our inferential analysis, we next feed that into the `generate()` verb. The `generate()` verb's main argument is `reps`, which is used to give how many different repetitions one would like to perform. Another argument here is `type`, which is automatically determined by the kinds of variables passed into `specify()`. We can also be explicit and set this `type` to be `type = "bootstrap"`. If you are not explicit, `infer` will send you a message just to make sure this is what you are wanting. This `type` argument will be further used in hypothesis testing in Chapter \@ref(hypothesis-testing) as well. Make sure to check out `?generate` to see the options here and use the `?` operator to better understand other verbs as well.

Let's `generate()` 1000 bootstrap samples:

```{r}
thousand_bootstrap_samples <- pennies_sample %>% 
  specify(response = year) %>% 
  generate(reps = 1000, type = "bootstrap")
```

We can use the `dplyr` `count()` function to help us understand what the `thousand_bootstrap_samples` data frame looks like:

```{r}
thousand_bootstrap_samples %>% 
  count(replicate)
```

Notice that each `replicate` has 50 entries here. Now that we have 1000 different bootstrap samples, our next step is to `calculate` the bootstrap statistics for each sample.

**Comparing back to original workflow**

Note that the steps up to this point of the `infer` pipeline produce the same procedure as what we saw before with `rep_sample_n()`. In other words, the following two code chunks produce similar results:

```{r eval=FALSE}
# With infer pipeline             # Without infer pipeline
pennies_sample %>%              pennies_sample %>% 
  specify(response = year) %>%      rep_sample_n(size = 50, 
   generate(reps = 1000)                         replace = TRUE, 
                                                 reps = 1000)
```


#### Calculate summary statistics {-}

```{r fig.align='center', echo=FALSE, out.width='70%'}
knitr::include_graphics("images/flowcharts/infer/calculate.png")
```

After `generate()`ing many different samples, we next want to condense those samples down into a single statistic for each `replicate`d sample. As seen in the diagram, the `calculate()` function is helpful here.

As we did at the beginning of this chapter, we now want to calculate the mean `year` for each bootstrap sample. To do so, we use the `stat` argument and set it to `"mean"` below. The `stat` argument has a variety of different options here and we will see further examples of this throughout the remaining chapters. 

```{r}
bootstrap_distribution <- pennies_sample %>% 
  specify(response = year) %>% 
  generate(reps = 1000) %>% 
  calculate(stat = "mean")
bootstrap_distribution
```

We see that the resulting data has 1000 rows and 2 columns corresponding to the 1000 replicates and the mean for each bootstrap sample.

**Comparing back to original workflow**

We can see that the `calculate()` step does what the `group_by() %>% summarize()` steps do in the original workflow: 

```{r eval=FALSE}
# With infer pipeline             # Without infer pipeline
pennies_sample %>%              pennies_sample %>% 
  specify(response = year) %>%      rep_sample_n(size = 50, replace = TRUE, 
  generate(reps = 1000) %>%                      reps = 1000) %>% 
  calculate(stat = "mean")          group_by(replicate) %>% 
                                    summarize(stat = mean(year))
```


#### Observed statistic / point estimate calculations {-}

Just as `group_by() %>% summarize()` produces a useful workflow in `dplyr`, we can also use `specify() %>% calculate()` to compute summary measures on our original sample data. It's often helpful both in confidence interval calculations, but also in hypothesis testing to identify what the corresponding statistic is in the original data. For our example on penny age, we computed above a value of `x_bar` using the `summarize()` verb in `dplyr`:

```{r}
pennies_sample %>% 
  summarize(stat = mean(year))
```

This can also be done by skipping the `generate()` step in the pipeline feeding `specify()` directly into `calculate()`:

```{r}
pennies_sample %>% 
  specify(response = year) %>% 
  calculate(stat = "mean")
```

This shortcut will be particularly useful when the calculation of the observed statistic is tricky to do using `dplyr` alone. This is particularly the case when working with more than one variable as will be seen in Chapter \@ref(hypothesis-testing).

#### Visualize the results {-}

```{r fig.align='center', echo=FALSE}
knitr::include_graphics("images/flowcharts/infer/visualize.png")
```

The `visualize()` verb provides a simple way to view the bootstrap distribution as a histogram of the `stat` variable values. It has many other arguments that one can use as well including the shading of the histogram values corresponding to the confidence interval values.

```{r eval=FALSE}
bootstrap_distribution %>% visualize()
# or
visualize(bootstrap_distribution)
```

```{r echo=FALSE}
bootstrap_distribution %>% visualize() +
  ggtitle("Simulation-Based Bootstrap Distribution")
```

The shape of this resulting distribution may look familiar to you.  It resembles the well-known normal (bell-shaped) curve.

The following diagram recaps the `infer` pipeline for creating a bootstrap distribution.

```{r echo=FALSE, purl=FALSE}
knitr::include_graphics("images/flowcharts/infer/ci_diagram.png")
```

### Building confidence intervals with the infer package {#infer-ci}

Recall how we showed two different methods for building a range of plausible values for an unknown parameter in Section \@ref(confidence-intervals). Let's now check out how the `infer` package and some new functions were used to get us there. There's also some additional functionality to further assist with visualizing the intervals built-in!


### The percentile method with infer {#percentile-method-infer}

Recall the percentile method of looking at the middle 95% of values with the lower endpoint at the 2.5^th^ percentile and the upper endpoint at the 97.5^th^ percentile. This can be done with `infer` using the `get_confidence_interval()` function. You can also use the alias `get_ci()` if you'd like the short version. That's what we use here.

```{r}
bootstrap_distribution %>% 
  get_ci(level = 0.95, type = "percentile")
```

These options are the default values for `level` and `type` so we can also just do:

```{r}
percentile_ci <- bootstrap_distribution %>% 
  get_ci()
percentile_ci
```

Now we see where the values obtained in Section \@ref(confidence-intervals) come from. Using the percentile method, our range of plausible values for the mean year of US pennies in circulation in 2019 is `r percentile_ci[["2.5%"]]` to `r percentile_ci[["97.5%"]]`. We can further use the `visualize()` function and the `shade_confidence_interval()` function, or alias `shade_ci()`, to view this. We use the `endpoints` argument to be those stored with name `percentile_ci`.

```{r eval=FALSE}
visualize(bootstrap_distribution) + 
  shade_ci(endpoints = percentile_ci)
```

```{r echo=FALSE}
# Will need to make a tweak to the {infer} package
# so that it doesn't always display "Null" here
visualize(bootstrap_distribution) + 
  ggtitle("Simulation-Based Bootstrap Distribution") +
  shade_ci(endpoints = percentile_ci)
```


You can see that 95% of the data stored in the `stat` variable in `bootstrap_distribution` falls between the two endpoints with 2.5% to the left outside of the shading and 2.5% to the right outside of the shading. The cut-off points that provide our range are shown with the darker lines.

Note that you can change the colors here as you wish using the `color` and `fill` arguments.

```{r eval=FALSE}
visualize(bootstrap_distribution) + 
  shade_ci(endpoints = percentile_ci,
           color = "brown",
           fill = "khaki")
```

```{r echo=FALSE}
# Will need to make a tweak to the {infer} package
# so that it doesn't always display "Null" here
visualize(bootstrap_distribution) + 
  ggtitle("Simulation-Based Bootstrap Distribution") +
  shade_ci(endpoints = percentile_ci,
           color = "brown",
           fill = "khaki")
```

```{r eval=FALSE}
visualize(bootstrap_distribution) + 
  shade_ci(endpoints = percentile_ci,
           color = "hotpink",
           fill = NULL)
```

```{r echo=FALSE}
# Will need to make a tweak to the {infer} package
# so that it doesn't always display "Null" here
visualize(bootstrap_distribution) + 
  ggtitle("Simulation-Based Bootstrap Distribution") +
  shade_ci(endpoints = percentile_ci,
           color = "hotpink",
           fill = NULL)
```


### The standard error method with infer

Recall the formula $\bar{x} \pm (multiplier * SE),$ where $\bar{x}$ is our original sample mean and $SE$ stands for **standard error** and corresponds to the standard deviation of the bootstrap distribution. This is the formula for using the standard error method for calculating a confidence interval.

The $multiplier$ is automatically calculated when `level` is provided with `level = 0.95` being the default. (95% of the values in a standard normal distribution fall within 1.96 standard deviations of the mean, so $multiplier = 1.96$ for `level = 0.95`, for example.)  As mentioned, this formula assumes that the bootstrap distribution is symmetric and bell-shaped. This is often the case with bootstrap distributions, especially those in which the original distribution of the sample is not highly skewed.

This $\bar{x} \pm (multiplier * SE)$ formula is implemented in the `get_ci()` function as shown with our pennies problem using the bootstrap distribution's variability as an approximation for the sampling distribution's variability. We'll see more on this approximation shortly.

Note that the center of the confidence interval (the `point_estimate`) must be provided for the standard error confidence interval.

```{r}
standard_error_ci <- bootstrap_distribution %>% 
  get_ci(type = "se", point_estimate = x_bar)
standard_error_ci
```

```{r eval=FALSE}
visualize(bootstrap_distribution) + 
  shade_ci(endpoints = standard_error_ci)
```

```{r echo=FALSE}
visualize(bootstrap_distribution) + 
  ggtitle("Simulation-Based Bootstrap Distribution") +
  shade_ci(endpoints = standard_error_ci)
```

As noted in Section \@ref(confidence-intervals) both methods produce similar confidence intervals.

| Percentile | Standard error |
|---|---|
| $[`r round(percentile_ci[["2.5%"]], 2)`, `r round(percentile_ci[["97.5%"]], 2)`]$ | $[`r round(standard_error_ci[["lower"]], 2)`, `r round(standard_error_ci[["upper"]],2)`]$ |

The `[lower, upper]` notation here corresponds to the `lower` value being the smallest included entry in the confidence interval and `upper` being the largest included entry in the confidence interval.



***



## Case study: Revisiting the red ball example {#one-prop-ci}

Let's revisit our exercise of trying to estimate the proportion of red balls in the bowl from Chapter \@ref(sampling). We are now interested in determining a confidence interval for the population parameter $p$, the proportion of balls that are red out of the total $N = 2400$ red and white balls. 

We will use the first sample reported from Ilyas and Yohan in Subsection \@ref(student-shovels) for our point estimate. They observed 21 red balls out of the 50 in their shovel. This data is stored in the `tactile_shovel_1` data frame in the `moderndive` package.


```{r}
tactile_shovel_1
```

### Observed statistic

To compute the proportion that are red in this data we can use the `specify() %>% calculate()` workflow. Note the use of the `success` argument here to clarify which of the two colors `"red"` or `"white"` we are interested in.

```{r}
p_hat <- tactile_shovel_1 %>% 
  specify(formula = color ~ NULL, success = "red") %>% 
  calculate(stat = "prop")
p_hat
```

### Bootstrap distribution for one proportion {#one-prop-boot}

Next, we want to calculate many different bootstrap samples and their corresponding bootstrap statistic (the proportion of red balls). We've done 1000 in the past, but let's go up to 10,000 now to better see the resulting distribution. Recall that this is done by including a `generate()` function call in the middle of our pipeline:

```{r eval=FALSE}
tactile_shovel_1 %>% 
  specify(formula = color ~ NULL, success = "red") %>% 
  generate(reps = 10000, type = "bootstrap")
```

```{r echo=FALSE}
set.seed(2000)
gen <- tactile_shovel_1 %>% 
  specify(formula = color ~ NULL, success = "red") %>% 
  generate(reps = 10000, type = "bootstrap")
```

This results in 50 rows for each of the 10,000 replicates. Lastly, we finish the `infer` pipeline by adding back in the `calculate()` step.

```{r eval=FALSE}
bootstrap_props <- tactile_shovel_1 %>% 
  specify(formula = color ~ NULL, success = "red") %>% 
  generate(reps = 10000, type = "bootstrap") %>% 
  calculate(stat = "prop")
```

```{r echo=FALSE}
if(!file.exists("rds/bootstrap_props.rds")){
  bootstrap_props <- gen %>% 
    calculate(stat = "prop")
  write_rds(bootstrap_props, "rds/bootstrap_props.rds")
}
if(file.exists("rds/bootstrap_props.rds")){
  bootstrap_props <- read_rds("rds/bootstrap_props.rds")
}
```

Let's `visualize()` what the resulting bootstrap distribution looks like as a histogram. We've adjusted the number of bins here as well to better see the resulting shape.

```{r eval=FALSE}
visualize(bootstrap_props, bins = 20)
```

```{r echo=FALSE}
# Will need to make a tweak to the {infer} package
# so that it doesn't always display "Null" here
visualize(bootstrap_props, bins = 20) + 
  ggtitle("Simulation-Based Bootstrap Distribution")
```

We see that the resulting distribution is symmetric and bell-shaped so it doesn't much matter which confidence interval method we choose. Let's use the standard error method to create a 95% confidence interval.

```{r}
standard_error_ci <- bootstrap_props %>% 
  get_ci(type = "se", level = 0.95, point_estimate = p_hat)
standard_error_ci
```

```{r eval=FALSE}
visualize(bootstrap_props, bins = 25) + 
  shade_ci(endpoints = standard_error_ci)
```

```{r echo=FALSE}
visualize(bootstrap_props, bins = 25) + 
  shade_ci(endpoints = standard_error_ci) +
  ggtitle("Simulation-Based Bootstrap Distribution")
```


We are "95% confident" that the true proportion of red balls in the bowl is between `r standard_error_ci[["lower"]]` and `r standard_error_ci[["upper"]]`. 

## Interpreting the confidence interval

One key to working with confidence intervals is to also understand how best to interpret them. From the previous example, this level of confidence (95%) is based on the standard error-based method including the true proportion 95% of the time if many different samples (not just the one we used) were collected and confidence intervals were created following this standard error-based method. Let's dig into what this means further by exploring the confidence intervals based on other samples to see how they compare to the one we just calculated. By the end of this section, you should have an understanding as to what "95% confident" means and how best to use that knowledge when you see that language used in other contexts.

As shown above in Subsection \@ref(one-prop-boot), one range of plausible values for the population proportion of red balls (the true proportion of all red balls in the entire bowl), denoted by $p$, is $[`r round(standard_error_ci[["lower"]], 2)`, `r round(standard_error_ci[["upper"]], 2)`]$. Recall that this confidence interval is based on bootstrapping using `tactile_shovel_1`. 


To best understand how to interpret a confidence interval, it is important to see how the process works when we have a known population parameter. Recall the `bowl` data frame in the `moderndive` package contains our population of interest. We can calculate the proportion of red balls in this population to get the value of $p$. Remember this isn't usually the case of knowing the population parameter, but we'll see why this is useful for our build-up shortly. Let's do this two ways to review both the `infer` and `dplyr` pipelines:

```{r}
bowl %>% 
  specify(formula = color ~ NULL, success = "red") %>% 
  calculate(stat = "prop")
```

```{r}
bowl %>% 
  summarize(stat = mean(color == "red"))
```

```{r echo=FALSE}
p_red <- mean(bowl$color == "red")
```


Both methods return `r p_red` as the proportion of red balls in the population of all balls in the bowl. So did our "95% confident" guess above of $[`r round(standard_error_ci[["lower"]], 2)`, `r round(standard_error_ci[["upper"]], 2)`]$ contain the "true value" for the population?

Yes, the population proportion (`r p_red`) does fall in this confidence interval. If we had a different sample of size 50 and constructed a confidence interval using the same method, would we be guaranteed that it contained the population parameter value as well? Let's try it out by pulling another sample from `bowl` of size 50:

```{r}
bowl_sample_2 <- bowl %>% 
  sample_n(size = 50)
```

Note the use of the `sample_n()` function in the `dplyr` package here. This does the same thing as `rep_sample_n(reps = 1)` but omits the extra `replicate` column.

We next create an `infer` pipeline to generate a standard error-based 95% confidence interval for $p$. Recall that we first need a `point_estimate` to act as the center of our standard error-based confidence interval. We calculate this with name `prop_red_2`.

```{r}
prop_red_2 <- bowl_sample_2 %>% 
  specify(formula = color ~ NULL, success = "red") %>% 
  calculate(stat = "prop")
standard_error_ci_2 <- bowl_sample_2 %>% 
  specify(formula = color ~ NULL, success = "red") %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "prop") %>% 
  get_ci(type = "se", point_estimate = prop_red_2)
standard_error_ci_2 
```

This new confidence interval also contains the value of $p$. Let's further investigate by repeating this process 100 times to get 100 different confidence intervals derived from 100 different samples of the population `bowl`. Each sample will have a size of 50 just as the original sample. We will plot each of these confidence intervals as horizontal lines. At the center of each confidence interval is the point estimate denoted by a dot. We will also show a red line corresponding to the known population value of `r p_red` red balls.

```{r reliable-se, fig.cap="Reliability of 95 percent confidence intervals",echo=FALSE}
set.seed(201)

ball_samples <- bowl %>% 
  rep_sample_n(size = 50, reps = 100, replace = FALSE)
nested_balls <- ball_samples %>% 
  group_by(replicate) %>% 
  tidyr::nest() %>% 
  mutate(sample_prop = purrr::map_dbl(data, ~mean(.x$color == "red")))
bootstrap_pipeline <- function(entry){
  entry %>% 
    specify(formula = color ~ NULL, success = "red") %>% 
    generate(reps = 1000, type = "bootstrap") %>% 
    calculate(stat = "prop")
}
if(!file.exists("rds/balls_se_cis.rds")){
  balls_se_cis <- nested_balls %>% 
    mutate(bootstraps = purrr::map(data, bootstrap_pipeline)) %>% 
    group_by(replicate) %>% 
    mutate(se_ci = purrr::map(bootstraps, get_ci, type = "se",
                              level = 0.95,
                              point_estimate = sample_prop))
  saveRDS(object = balls_se_cis, "rds/balls_se_cis.rds")
} else {
  balls_se_cis <- readRDS("rds/balls_se_cis.rds")
}
se_cis <- balls_se_cis %>% 
  tidyr::unnest(se_ci) %>% 
  mutate(captured = lower <= p_red & p_red <= upper)
ggplot(se_cis) +
  geom_point(aes(x = sample_prop, y = replicate, color = captured)) +
  geom_segment(aes(y = replicate, yend = replicate, x = lower, xend = upper, 
                   color = captured)) +
  labs(
    x = expression("Proportion of red balls"),
    y = "Replicate ID",
    title = expression(paste(
      "95% standard error-based confidence intervals for ", p, sep = "")
      )
  ) +
  scale_color_manual(values = c("blue", "orange")) + 
  geom_vline(xintercept = p_red, color = "red") 
```

Of the 100 confidence intervals based on samples of size $n = 50$, `r sum(se_cis[["captured"]])` of them captured the population mean $p = `r p_red`$, whereas `r 100 - sum(se_cis[["captured"]])` of them did not include it. If we repeated this process of building confidence intervals more times with more samples, we'd expect 95% of them to contain the population parameter $p$. In other words, the procedure we have used to generate confidence intervals is "95% reliable" in that we can expect it to include the true population parameter 95% of the time if the process is repeated.

To further accentuate this point, let's perform a similar procedure using 90% confidence intervals instead. This time we will use the percentile method instead of the standard error method for computing the confidence intervals.

```{r reliable-perc, fig.cap="Reliability of 90 percent confidence intervals", echo=FALSE}
set.seed(201)
balls_samples2 <- bowl %>% 
  rep_sample_n(size = 50, reps = 100, replace = FALSE)
nested_balls2 <- balls_samples2 %>% 
  group_by(replicate) %>% 
  tidyr::nest()
infer_pipeline <- function(entry){
  entry %>% 
    specify(formula = color ~ NULL, success = "red") %>% 
    generate(reps = 1000, type = "bootstrap") %>% 
    calculate(stat = "prop") %>% 
    get_ci(level = 0.9)
}
if(!file.exists("rds/balls_perc_cis.rds")){
  balls_perc_cis <- nested_balls2 %>% 
    mutate(percentile_ci = purrr::map(data, infer_pipeline)) %>% 
    mutate(point_estimate = purrr::map_dbl(data, ~mean(.x$color == "red")))
  write_rds(balls_perc_cis, "rds/balls_perc_cis.rds")
} else {
  balls_perc_cis <- read_rds("rds/balls_perc_cis.rds")
}
perc_cis <- balls_perc_cis %>% 
  tidyr::unnest(percentile_ci) %>% 
  rename(lower = `5%`, upper = `95%`) %>% 
  mutate(captured = lower <= p_red & p_red <= upper)
ggplot(perc_cis) +
  geom_point(aes(x = point_estimate, y = replicate, color = captured)) +
  geom_segment(aes(y = replicate, yend = replicate, x = lower, xend = upper, 
                   color = captured)) +
  labs(
    x = expression("Proportion of red balls"),
    y = "Replicate ID",
    title = expression(paste("90% percentile-based confidence intervals for ", 
                             p, sep = ""))
  ) +
  scale_color_manual(values = c("blue", "orange")) + 
  geom_vline(xintercept = p_red, color = "red") 
```

Of the 100 confidence intervals based on samples of size $n = 50$, `r sum(perc_cis[["captured"]])` of them captured the population proportion $p = `r p_red`$, whereas `r 100 - sum(perc_cis[["captured"]])` of them did not include it. Repeating this process for more samples would result in us getting closer and closer to 90% of the confidence intervals including the true value. It is common to say while interpreting a confidence interval to be "95% confident" or "90% confident" that the true value falls within the range of the specified confidence interval. We will use this "confident" language throughout the rest of this chapter, but remember that it has more to do with a measure of the reliability of the building process.

#### Back to our pennies example {-}

After this elaboration on what the level corresponds to in a confidence interval, let's conclude by providing an interpretation of the original confidence interval result we found in Subsection \@ref(infer-ci).

**Interpretation:** We are 95% confident that the true mean year of pennies in circulation in 2019 is between `r percentile_ci[["2.5%"]]` to `r percentile_ci[["97.5%"]]`. This level of confidence is based on the percentile-based method including the true mean 95% of the time if many different samples (not just the one we used) were collected and confidence intervals were created.

### The width of confidence intervals {-}

#### The impact of confidence levels {-}

When looking at the relative sizes of the orange horizontal lines in Figure \@ref(fig:reliable-se) with 95% confidence intervals and Figure \@ref(fig:reliable-perc) with 90% confidence intervals, does anything stand out in terms of the width of the intervals to you? The statement of confidence in terms of the level should match with what you expect of the word "confident." If someone says they are 99% confident about the high temperature between one value and another for the next day, we'd expect that range to be higher than if they said they were only 80% confident, right?

To elaborate on this a bit, if we wanted to make a guess as to what the forecasted summertime high temperature in Brussels, Belgium would be for a day in July, we could say pretty confidently that the high temperature wouldn't be below 55&deg; F (approximately 13&deg; C) and that it wouldn't be above 72&deg; F (approximately 22&deg; C). Let's say we are 90% confident for a given day about this claim. What would we need to do to this range to increase our level of confidence?

We'd need to increase it! To be more confident about the range of plausible values for our high temperature, we need to add in more possible temperatures since we might have a cold streak or an unseasonably warm day. 

What if we wanted to be a little less confident and say have a 50-50 chance of guessing at what the high temperature would be. Well, if we are OK being wrong 50% of the time, we could guess something in a much narrower range for plausible values of the high temperature. Something like [61&deg; F, 66&deg; F] (approximately [16&deg; C, 19&deg; C]) might be a 50% confident guess, say. By narrowing our range, we've decreased our level of confidence. This analogy relates well (maybe not exactly) to confidence intervals in statistics.

**Higher confidence levels tend to produce wider confidence intervals.**

Let's play with do a little more analysis using the `bowl` data to construct 80%, 95%, and 99% confidence intervals here to drill this idea home. We'll focus on the percentile-based method though a similar analysis could be done for the standard error-based method. Let's calculate 100 confidence intervals of each of these three different levels and then look at the median and mean length of these intervals. These will be stored in the `perc_cis_by_level` data frame. 

<!-- Albert: Should we load these into the moderndive package too so that readers can explore them a bit? -->

```{r perc-sizes, echo=FALSE}
balls_samples2 <- bowl %>% 
  rep_sample_n(size = 50, reps = 100, replace = FALSE)
nested_balls2 <- balls_samples2 %>% 
  group_by(replicate) %>% 
  tidyr::nest()
infer_pipeline <- function(entry, ci_level){
  entry %>% 
    specify(formula = color ~ NULL, success = "red") %>% 
    generate(reps = 1000, type = "bootstrap") %>% 
    calculate(stat = "prop") %>% 
    get_ci(level = ci_level)
}
if(!file.exists("rds/balls_perc_cis_80.rds")){
  balls_perc_cis_80 <- nested_balls2 %>% 
    mutate(percentile_ci = purrr::map(data, infer_pipeline, ci_level = 0.8)) %>% 
    mutate(point_estimate = purrr::map_dbl(data, ~mean(.x$color == "red")))
  write_rds(balls_perc_cis_80, "rds/balls_perc_cis_80.rds")
} else {
  balls_perc_cis_80 <- read_rds("rds/balls_perc_cis_80.rds")
}
perc_cis_80 <- balls_perc_cis_80 %>% 
  tidyr::unnest(percentile_ci) %>% 
  rename(lower = `10%`, upper = `90%`) %>% 
  select(-data) %>% 
  mutate(confidence_level = 80)

if(!file.exists("rds/balls_perc_cis_95.rds")){
  balls_perc_cis_95 <- nested_balls2 %>% 
    mutate(percentile_ci = purrr::map(data, infer_pipeline, ci_level = 0.95)) %>% 
    mutate(point_estimate = purrr::map_dbl(data, ~mean(.x$color == "red")))
  write_rds(balls_perc_cis_95, "rds/balls_perc_cis_95.rds")
} else {
  balls_perc_cis_95 <- read_rds("rds/balls_perc_cis_95.rds")
}
perc_cis_95 <- balls_perc_cis_95 %>% 
  tidyr::unnest(percentile_ci) %>% 
  rename(lower = `2.5%`, upper = `97.5%`) %>% 
  select(-data) %>% 
  mutate(confidence_level = 95)

if(!file.exists("rds/balls_perc_cis_99.rds")){
  balls_perc_cis_99 <- nested_balls2 %>% 
    mutate(percentile_ci = purrr::map(data, infer_pipeline, ci_level = 0.99)) %>% 
    mutate(point_estimate = purrr::map_dbl(data, ~mean(.x$color == "red")))
  write_rds(balls_perc_cis_99, "rds/balls_perc_cis_99.rds")
} else {
  balls_perc_cis_99 <- read_rds("rds/balls_perc_cis_99.rds")
}
perc_cis_99 <- balls_perc_cis_99 %>% 
  tidyr::unnest(percentile_ci) %>% 
  rename(lower = `0.5%`, upper = `99.5%`) %>% 
  select(-data) %>% 
  mutate(confidence_level = 99)

percentile_cis_by_level <- bind_rows(perc_cis_80, 
                                     perc_cis_95, 
                                     perc_cis_99)
```

Let's take a look into what the `perc_cis_by_level` data frame looks like and how a sample of 10 different confidence intervals each from the 80%, 95%, and 99% levels compare visually in terms of length. Then, we'll start computing some widths of the confidence intervals. Then we'll head into calculating the mean and median widths across the three different levels.

```{r perc-cis-level-print, echo=FALSE}
percentile_cis_by_level %>% 
  sample_n(10) %>% 
  kable(
    digits = 3,
    caption = "10 randomly sampled confidence intervals for p for varying confidence levels", 
    booktabs = TRUE,
    longtable = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position", "repeat_header"))
```

```{r echo=FALSE}
sample_of_cis <- percentile_cis_by_level %>% 
  group_by(confidence_level) %>% 
  sample_n(10) %>% 
  mutate(sample_row = 1:10)
ggplot(sample_of_cis) +
  geom_point(aes(x = point_estimate, y = sample_row)) +
  geom_segment(aes(y = sample_row, yend = sample_row, x = lower, xend = upper)) +
  labs(
    x = expression("Proportion of red balls"),
    y = "Row of sample (out of 10)",
    title = expression(paste("90% percentile-based confidence intervals for ", 
                             p, " by level", sep = ""))
  ) +
  scale_y_continuous(breaks = 1:10) +
  facet_wrap(~ confidence_level)
```



We see that the sample proportion of reds varies in the `point_estimate` column with varying `lower` and `upper` bounds as well depending on the variability of the bootstrap distribution. The width of the confidence intervals appears to increase from left to right going from 80% confidence levels to 95% and then to 99%. Let's now compute the confidence interval (CI) width for each of these intervals and then get the median and mean length.


```{r}
percentile_cis_by_level %>% 
  mutate(width = upper - lower) %>% 
  group_by(confidence_level) %>% 
  summarize(median_width = median(width),
            mean_width = mean(width))
```

As expected, as the confidence level increases, the width of the corresponding confidence interval also increases. To be more confident, we need to increase the range of plausible values.

#### The impact of sample size {-}

We can also observe the impact of sample size on these calculations and make some generalizations. You'll see in Subsection \@ref(theory-ci) some reasons via mathematical formulas for the behavior of confidence interval width and changing sample sizes too.

Let's hold the confidence level fixed at 90% using the percentile-based method, but take samples of size 25, 50, and 100 corresponding to the different sizes of the shovels available to us in Chapter \@ref(sampling). Do you expect smaller sample sizes to produce wider confidence intervals? Or should larger sample sizes produce wider ones? Let's investigate.

Recall the `virtual_samples_25`, `virtual_samples_50`, and `virtual_samples_100` data frames that were calculated in Chapter \@ref(sampling). As a reminder, here's the code needed to compute them.

```{r eval=FALSE}
virtual_samples_25 <- bowl %>% 
  rep_sample_n(size = 25, reps = 1000, replace = FALSE)
balls_samples_50 <- bowl %>% 
  rep_sample_n(size = 50, reps = 1000, replace = FALSE)
balls_samples_100 <- bowl %>% 
  rep_sample_n(size = 100, reps = 1000, replace = FALSE)
```

```{r echo=FALSE}
virtual_samples_25 <- read_rds("rds/virtual_samples_25.rds")
virtual_samples_50 <- read_rds("rds/virtual_samples_50.rds")
virtual_samples_100 <- read_rds("rds/virtual_samples_100.rds")
```



```{r perc-sample-sizes, echo=FALSE}
nested_balls_25 <- virtual_samples_25 %>% 
  group_by(replicate) %>% 
  tidyr::nest()

nested_balls_50 <- virtual_samples_50 %>% 
  group_by(replicate) %>% 
  tidyr::nest()

nested_balls_100 <- virtual_samples_100 %>% 
  group_by(replicate) %>% 
  tidyr::nest()

infer_pipeline <- function(entry, ci_level){
  entry %>% 
    specify(formula = color ~ NULL, success = "red") %>% 
    generate(reps = 1000, type = "bootstrap") %>% 
    calculate(stat = "prop") %>% 
    get_ci(level = 0.9)
}
if(!file.exists("rds/balls_perc_cis_n_25.rds")){
  balls_perc_cis_n_25 <- nested_balls_25 %>% 
    mutate(percentile_ci = purrr::map(data, infer_pipeline)) %>% 
    mutate(point_estimate = purrr::map_dbl(data, ~mean(.x$color == "red")))
  write_rds(balls_perc_cis_n_25, "rds/balls_perc_cis_n_25.rds")
} else {
  balls_perc_cis_n_25 <- read_rds("rds/balls_perc_cis_n_25.rds")
}
perc_cis_n_25 <- balls_perc_cis_n_25 %>% 
  tidyr::unnest(percentile_ci) %>% 
  rename(lower = `5%`, upper = `95%`) %>% 
  select(-data) %>%
  mutate(sample_size = 25)

if(!file.exists("rds/balls_perc_cis_n_50.rds")){
  balls_perc_cis_n_50 <- nested_balls_50 %>% 
    mutate(percentile_ci = purrr::map(data, infer_pipeline)) %>% 
    mutate(point_estimate = purrr::map_dbl(data, ~mean(.x$color == "red")))
  write_rds(balls_perc_cis_n_50, "rds/balls_perc_cis_n_50.rds")
} else {
  balls_perc_cis_n_50 <- read_rds("rds/balls_perc_cis_n_50.rds")
}
perc_cis_n_50 <- balls_perc_cis_n_50 %>% 
  tidyr::unnest(percentile_ci) %>% 
  rename(lower = `5%`, upper = `95%`) %>% 
  select(-data) %>%
  mutate(sample_size = 50)

if(!file.exists("rds/balls_perc_cis_n_100.rds")){
  balls_perc_cis_n_100 <- nested_balls_100 %>% 
    mutate(percentile_ci = purrr::map(data, infer_pipeline)) %>% 
    mutate(point_estimate = purrr::map_dbl(data, ~mean(.x$color == "red")))
  write_rds(balls_perc_cis_n_100, "rds/balls_perc_cis_n_100.rds")
} else {
  balls_perc_cis_n_100 <- read_rds("rds/balls_perc_cis_n_100.rds")
}
perc_cis_n_100 <- balls_perc_cis_n_100 %>% 
  tidyr::unnest(percentile_ci) %>% 
  rename(lower = `5%`, upper = `95%`) %>% 
  select(-data) %>%
  mutate(sample_size = 100)

percentile_cis_by_n <- bind_rows(perc_cis_n_25, perc_cis_n_50, perc_cis_n_100)
```

<!-- Albert: What about these confidence intervals for different sample sizes in the moderndive package? -->

As we did when investigating the role of confidence level, confidence intervals for each of the different 1000 samples of each of these three sample sizes has been saved into the `percentile_cis_by_n` data frame. Let's investigate width visually first and then look at the median and mean length of these intervals.

```{r echo=FALSE}
sample_of_cis <- percentile_cis_by_n %>% 
  group_by(sample_size) %>% 
  sample_n(10) %>% 
  mutate(sample_row = 1:10)
ggplot(sample_of_cis) +
  geom_point(aes(x = point_estimate, y = sample_row)) +
  geom_segment(aes(y = sample_row, yend = sample_row, x = lower, xend = upper)) +
  labs(
    x = expression("Proportion of red balls"),
    y = "Row of sample (out of 10)",
    title = expression(paste("90% percentile-based confidence intervals for ", 
                             p, " by sample size", sep = ""))
  ) +
  scale_y_continuous(breaks = 1:10) +
  facet_wrap(~ sample_size)
```


```{r}
percentile_cis_by_n %>% 
  mutate(width = upper - lower) %>% 
  group_by(sample_size) %>% 
  summarize(median_width = median(width),
            mean_width = mean(width))
```

So as the sample size increases the width of our confidence intervals decreases. This intuitively makes sense since as we have larger samples we are getting closer and closer to the actual size of the population. As we get closer to the actual size of the population, we will have less and less variability in the sample proportion red since there will be less and less variability in the samples pulled from the population.

<!-- A good learning check might be to have the readers calculate confidence intervals when n = 1000, 2000, 2400. To their astonishment (maybe), they'll see that the size of the confidence interval is 0 when they get to 2400. -->

***

## Case study: Comparing two proportions {#case-study-two-prop-ci}

Let's now look into another example where the `infer` pipeline really shows off its power by looking at two variables. We'll have a response and an explanatory variable instead of just the one variable we've seen so far in the bowl of balls and pennies examples.

If you see someone else yawn, are you more likely to yawn? In an [episode](http://www.discovery.com/tv-shows/mythbusters/mythbusters-database/yawning-contagious/) of the show *Mythbusters*, they tested the myth that yawning is contagious. The snippet from the show is available to view in the United States on the Discovery Network website [here](https://www.discovery.com/tv-shows/mythbusters/videos/is-yawning-contagious). More information about the episode is also available on IMDb [here](https://www.imdb.com/title/tt0768479/).

Fifty adults who thought they were being considered for an appearance on the show were interviewed by a show recruiter ("confederate") who either yawned or did not. Participants then sat by themselves in a large van and were asked to wait. While in the van, the Mythbusters watched via hidden camera to see if the unaware participants yawned. The data frame containing the results is available at `mythbusters_yawn` in the `moderndive` package. Let's check it out.

```{r}
mythbusters_yawn
```

- The participant ID is stored in the `subj` variable with values of 1 to 50.
- The `group` variable is either `"seed"` for when a confederate was trying to influence the participant or `"control"` if a confederate did not interact with the participant.
- The `yawn` variable is either `"yes"` if the participant yawned or `"no"` if the participant did not yawn.

We can use the `janitor` package to get a glimpse into this data in a table format:

```{r}
mythbusters_yawn %>% 
  tabyl(group, yawn) %>% 
  adorn_percentages() %>% 
  adorn_pct_formatting() %>% 
  # To show original counts
  adorn_ns()
```

We are interested in comparing the proportion of those that yawned after seeing a seed versus those that yawned with no seed interaction. We'd like to see if the difference between these two proportions is significantly larger than 0. If so, we'd have evidence to support the claim that yawning is contagious based on this study.

In looking over this problem, we can make note of some important details to include in our `infer` pipeline:

- We are calling a `success` having a `yawn` value of `"yes"`.
- Our response variable will always correspond to the variable used in the `success` so the response variable is `yawn`.
- The explanatory variable is the other variable of interest here: `group`.

To summarize, we are looking to see the examine the relationship between yawning and whether or not the participant saw a seed yawn or not.

### Compute the point estimate

```{r eval=FALSE}
mythbusters_yawn %>% 
  specify(formula = yawn ~ group)
```

```
Error: A level of the response variable `yawn` needs to be specified
for the `success` argument in `specify()`.
```

Note that the `success` argument must be specified in situations such as this where the response variable has only two levels.

```{r}
mythbusters_yawn %>% 
  specify(formula = yawn ~ group, success = "yes")
```

We next want to calculate the statistic of interest for our sample. This corresponds to the difference in the proportion of successes.

```{r eval=FALSE}
mythbusters_yawn %>% 
  specify(formula = yawn ~ group, success = "yes") %>% 
  calculate(stat = "diff in props")
```

```
Error: Statistic is based on a difference; specify the `order` in which to
subtract the levels of the explanatory variable.
```


We see another error here. To further check to make sure that R knows exactly what we are after, we need to provide the `order` in which R should subtract these proportions of successes. As the error message states, we'll want to put `"seed"` first after `c()` and then `"control"`: `order = c("seed", "control")`. Our point estimate is thus calculated:

```{r}
obs_diff <- mythbusters_yawn %>% 
  specify(formula = yawn ~ group, success = "yes") %>% 
  calculate(stat = "diff in props", order = c("seed", "control"))
obs_diff
```

This value represents the proportion of those that yawned after seeing a seed yawn (0.2941) minus the proportion of those that yawned with not seeing a seed (0.25).

### Bootstrap distribution

Our next step in building a confidence interval is to create a bootstrap distribution of statistics (differences in proportions of successes). We saw how it works with both a single variable in computing bootstrap means in Subsection \@ref(bootstrap-process) and in computing bootstrap proportions in Section \@ref(one-prop-ci), but we haven't yet worked with bootstrapping involving multiple variables though. 

In the `infer` package, bootstrapping with multiple variables means that each **row** is potentially resampled. Let's investigate this by looking at the first few rows of `mythbusters_yawn`:

```{r}
head(mythbusters_yawn)
```

When we bootstrap this data, we are potentially pulling the subject's readings multiple times. Thus, we could see the entries of `"seed"` for `group` and `"no"` for `yawn` together in a new row in a bootstrap sample. This is further seen by exploring the `sample_n()` function in `dplyr` on this smaller 6-row data frame comprised of `head(mythbusters_yawn)`. The `sample_n()` function can perform this bootstrapping procedure and is similar to the `rep_sample_n()` function in `infer`, except that it is not `rep`eated but rather only performs one sample with or without replacement.

```{r}
head(mythbusters_yawn) %>% 
  sample_n(size = 6, replace = TRUE)
```

We can see that in this bootstrap sample generated from the first six rows of `mythbusters_yawn`, we have some rows repeated. The same is true when we perform the `generate()` step in `infer` as done below.


```{r}
bootstrap_distribution <- mythbusters_yawn %>% 
  specify(formula = yawn ~ group, success = "yes") %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "diff in props", order = c("seed", "control"))
```

<!-- A challenging learning check for those {dplyr} diehards is to get these values without using {infer}. It takes a double group_by() and some trickery, but
could be a good exercise for those that don't quite see the power of {infer}. -->

```{r eval=FALSE}
bootstrap_distribution %>% 
  visualize(bins = 20)
```

```{r echo=FALSE}
bootstrap_distribution %>% 
  visualize(bins = 20) +
  ggtitle("Simulation-Based Bootstrap Distribution")
```


This distribution is roughly symmetric and bell-shaped but isn't quite there. Let's use the percentile-based method to compute a 95% confidence interval for the true difference in the proportion of those that yawn with and without a seed presented. The arguments are explicitly listed here but remember they are the defaults and simply `get_ci()` can be used.

```{r}
bootstrap_distribution %>% 
  get_ci(type = "percentile", level = 0.95)
```

```{r include=FALSE}
myth_ci <- bootstrap_distribution %>% 
  get_ci(type = "percentile", level = 0.95)
```


The confidence interval shown here includes the value of 0. We'll see in Chapter \@ref(hypothesis-testing) further what this means in terms of this difference being statistically significant or not, but let's examine a bit here first. The range of plausible values for the difference in the proportion of those that yawned with and without a seed is between `r myth_ci[["2.5%"]]` and `r myth_ci[["97.5%"]]`. 

Therefore, we are not sure which proportion is larger. Some of the bootstrap statistics showed the proportion without a seed to be higher and others showed the proportion with a seed to be higher. If the confidence interval was entirely above zero, we would be relatively sure (about "95% confident") that the seed group had a higher proportion of yawning than the control group.

Note that this all relates to the importance of denoting the `order` argument in the `calculate()` function. Since we specified `"seed"` and then `"control"` positive values for the statistic correspond to the `"seed"` proportion being higher, whereas negative values correspond to the `"control"` group being higher.

We, therefore, have evidence via this confidence interval suggesting that the conclusion from the Mythbusters show that "yawning is contagious" being "confirmed" is not statistically appropriate.

<!-- Good learning check is to ask for an interpretation using the format above for this Mythbusters example. -->

---

## Conclusion {#ci-conclusion}

### Comparing bootstrap and sampling distributions

Earlier in this chapter, we mentioned that the variability of the sampling distribution is often well-approximated by the variability of the bootstrap distribution. Since we've computed both of these distributions for the `bowl` example, let's dig into both of them further to make comparisons.

#### Sampling distribution {-}

Let's assume that `bowl` represents our population of interest. We'll next go over again how to create a sampling distribution for the population proportion of red balls, denoted by $p$, using the `rep_sample_n()` function seen in Chapter \@ref(sampling). Let's use a mega-virtual shovel of size 200 here. First, we will create 1000 samples from the `bowl` data frame.

```{r}
thousand_samples <- bowl %>% 
  rep_sample_n(size = 200, reps = 1000, replace = FALSE)
```

When creating a sampling distribution, we do not replace the items when we create each sample. This is in contrast to the bootstrap distribution. It's important to remember that the sampling distribution is sampling **without** replacement from the population to better understand sample-to-sample variability, whereas the bootstrap distribution is sampling **with** replacement from our original sample to better understand potential sample-to-sample variability. For the sampling distribution, we have access to the population whereas with the bootstrap distribution we are only going to pull ourselves up from our bootstraps using the single sample.

After sampling from `bowl` 1000 times, we next want to compute the proportion of red balls for each of the 1000 samples:

```{r}
sampling_distribution <- thousand_samples %>% 
  group_by(replicate) %>% 
  summarize(stat = mean(color == "red"))
```


```{r, fig.cap="Sampling distribution for proportion red for n=200 samples of balls"}
ggplot(sampling_distribution, aes(x = stat)) +
  geom_histogram(bins = 10, fill = "salmon", color = "white")
```

We can also examine the variability in this sampling distribution by calculating the standard deviation of the `stat` column. Remember that the standard deviation of the sampling distribution is the **standard error**, frequently denoted as `se`.

```{r}
sampling_distribution %>% 
  summarize(se = sd(stat))
```


#### Bootstrap distribution {-}

Let's now see how the shape of the bootstrap distribution compares to that of the sampling distribution. We'll shade the bootstrap distribution blue to further assist with remembering which is which, with the sampling distribution shaded salmon color. Let's walk through the steps needed with the `infer` pipeline to create the bootstrap distribution. We first need a sample of size 200 pulled from the `bowl` to give us a starting sample:

```{r}
sample_200 <- bowl %>% 
  sample_n(200, replace = FALSE)
```


1. `specify` variables

We first identify which variable(s) we are interested in for our inferential analysis.

```{r eval=FALSE}
sample_200 %>% 
  specify(formula = color ~ NULL)
```

```
Error: A level of the response variable `color` needs to be specified for the
`success` argument in `specify()`.
```


The `infer` package sends an error here that we need to tell it which of the possible `color` options we'd like to call a `success.`

```{r eval=FALSE}
sample_200 %>% 
  specify(formula = color ~ NULL, success = "red")
```

```{r echo=FALSE}
set.seed(2019)
spec_200 <- sample_200 %>% 
  specify(formula = color ~ NULL, success = "red")
spec_200
```

2. `generate` bootstrap replicates

```{r eval=FALSE}
sample_200 %>% 
  specify(formula = color ~ NULL, success = "red") %>% 
  generate(reps = 1000, type = "bootstrap")
```

```{r echo=FALSE}
gen_200 <- sample_200 %>% 
  specify(formula = color ~ NULL, success = "red") %>% 
  generate(reps = 1000, type = "bootstrap")
gen_200
```

3. `calculate` statistics

```{r}
bootstrap_distribution_n_200 <- sample_200 %>% 
  specify(formula = color ~ NULL, success = "red") %>% 
  generate(reps = 1000, type = "bootstrap") %>% 
  calculate(stat = "prop")
bootstrap_distribution_n_200
```


4. `visualize` distribution of `stat`istics

```{r eval=FALSE}
visualize(bootstrap_distribution_n_200, bins = 10, fill = "blue")
```

```{r echo=FALSE}
visualize(bootstrap_distribution_n_200, bins = 10, fill = "blue") +
  ggtitle("Simulation-Based Bootstrap Distribution")
```


*Side-by-side*

Now that we have both the sampling distribution and the bootstrap distribution, let's put them on the same scales and examine their variability both visually and also by computing relevant standard deviations.

```{r fig.cap="Comparing sampling and bootstrap distributions", echo=FALSE}
p_samp <- ggplot(sampling_distribution, aes(x = stat)) +
  geom_histogram(bins = 10, fill = "salmon", color = "white") +
  coord_cartesian(xlim = seq(0.25, 0.50, 0.05), ylim = seq(0, 300, 50)) +
  labs(title = "Sampling distribution for n = 200")
p_boot <- ggplot(bootstrap_distribution_n_200, aes(x = stat)) +
  geom_histogram(bins = 10, fill = "blue", color = "white") +
  coord_cartesian(xlim = seq(0.25, 0.50, 0.05), ylim = seq(0, 300, 50)) +
  labs(title = "Bootstrap distribution for n = 200")
p_samp + p_boot
```


```{r}
sampling_distribution %>% 
  summarize(se = sd(stat))
```

```{r}
bootstrap_distribution_n_200 %>% 
  summarize(se = sd(stat))
```

Notice that the bootstrap distribution's standard deviation is a good approximation for the standard error, the standard deviation of the sampling distribution. Note that while the standard deviations are similar, the center of the sampling distribution and the bootstrap distribution differ:

```{r}
sampling_distribution %>% 
  summarize(mean_of_sampling_means = mean(stat))
```

```{r}
bootstrap_distribution_n_200 %>% 
  summarize(mean_of_bootstrap_means = mean(stat))
```

Since the bootstrap distribution is centered at the original sample proportion, it doesn't necessarily provide a good estimate of the overall population proportion $p$, which we calculated to be `r p_red`. Notice that this value matches up well with the mean of the sampling distribution. This is actually an artifact of the Central Limit Theorem introduced in Chapter \@ref(sampling). The mean of the sampling distribution is expected to be the mean of the overall population.

The unfortunate fact though is that we don't know the population mean in nearly all circumstances. The motivation of presenting it here was to show that the theory behind the Central Limit Theorem works using the tools you've worked with so far using the `ggplot2`, `dplyr`, `moderndive`, and `infer` packages.

If we aren't able to use the sample mean as a good guess for the population mean, how should we best go about estimating what the population mean may be if we can only select samples from the population. We've now come full circle and can discuss the underpinnings of the confidence interval and ways to interpret it.

---

### Theory-based confidence intervals {#theory-ci}

When the bootstrap distribution has the nice symmetric, bell shape that we saw in the red balls example above, we can also use a formula to quantify the standard error. This provides another way to compute a confidence interval but is a little more tedious and mathematical. The steps are outlined below. We've also shown how we can use the confidence interval (CI) interpretation in this case as well to support your understanding of this tricky concept.

#### Procedure for building a theory-based CI for $p$ {-}

To construct a theory-based confidence interval for $p$, the unknown true population proportion we

1. Collect a sample of size $n$
1. Compute $\widehat{p}$
1. Compute the standard error $$\text{SE} = \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}$$
1. Compute the margin of error $$\text{MoE} = 1.96 \cdot \text{SE} =  1.96 \cdot \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}$$
1. Compute both end points of the confidence interval:
    + The lower end point `lower_ci`: $$\widehat{p} - \text{MoE} = \widehat{p} - 1.96 \cdot \text{SE} = \widehat{p} - 1.96 \cdot \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}$$
    + The upper end point `upper_ci`: $$\widehat{p} + \text{MoE} = \widehat{p} + 1.96 \cdot \text{SE} = \widehat{p} + 1.96 \cdot \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}$$
1. Alternatively, you can succinctly summarize a 95% confidence interval for $p$ using the $\pm$ symbol:

$$
\widehat{p} \pm \text{MoE} = \widehat{p} \pm 1.96 \cdot \text{SE} = \widehat{p} \pm 1.96 \cdot \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}
$$


#### Confidence intervals based on 33 tactile samples {-}

Let's load the tactile sampling data for the 33 groups from Chapter \@ref(sampling). Recall this data was saved in the `tactile_prop_red` data frame included in the `moderndive` package. 

<!-- Albert: Load tactile_prop_red into moderndive package too? -->

```{r, eval=FALSE, message=FALSE, warning=FALSE}
tactile_prop_red
```

Let's now apply the above procedure for constructing confidence intervals for $p$ using the data saved in `tactile_prop_red` by adding/modifying new columns using the `dplyr` package data wrangling tools seen in Chapter \@ref(wrangling):

1. Rename `prop_red` to `p_hat`, the official name of the sample proportion
1. Make explicit the sample size `n` of $n$ = 50
1. the standard error `SE`
2. the margin of error `MoE`
3. the left endpoint of the confidence interval `lower_ci`
4. the right endpoint of the confidence interval `upper_ci`

```{r, eval=FALSE, message=FALSE, warning=FALSE}
conf_ints <- tactile_prop_red %>% 
  rename(p_hat = prop_red) %>% 
  mutate(
    n = 50,
    SE = sqrt(p_hat * (1 - p_hat) / n),
    MoE = 1.96 * SE,
    lower_ci = p_hat - MoE,
    upper_ci = p_hat + MoE
  )
conf_ints
```
```{r, echo=FALSE, message=FALSE, warning=FALSE}
conf_ints <- tactile_prop_red %>% 
  rename(p_hat = prop_red) %>% 
  select(-replicate) %>% 
  mutate(
    n = 50, 
    SE = sqrt(p_hat*(1-p_hat)/n),
    MoE = 1.96*SE,
    lower_ci = p_hat - MoE,
    upper_ci = p_hat + MoE,
    y = seq_len(n())
  )
conf_ints %>% 
  select(-y) %>% 
  kable(
    digits = 3,
    caption = "33 confidence intervals from 33 tactile samples of size n=50", 
    booktabs = TRUE,
    longtable = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position", "repeat_header", 
                                  "scale_down"))
```

Let's plot:

1. These 33 confidence intervals for $p$: from `lower_ci` to `upper_ci`
1. The true population proportion $p = 900 / 2400 = 0.375$ with a red vertical line

```{r tactile-conf-int, echo=FALSE, message=FALSE, warning=FALSE, fig.cap= "33 confidence intervals based on 33 tactile samples of size n=50", fig.height=6}
groups <- conf_ints$group
conf_ints %>%
  mutate(p = 900 / 2400,
         captured = lower_ci <= p & p <= upper_ci) %>%
  ggplot() +
  geom_point(aes(x = p_hat, y = y, col = captured)) +
  geom_vline(xintercept = 900 / 2400, col = "red") +
  geom_segment(aes(
    y = y,
    yend = y,
    x = lower_ci,
    xend = upper_ci,
    col = captured
  )) +
  scale_y_continuous(breaks = 1:33, labels = groups) +
  labs(x = expression("Proportion red"),
       y = "",
       title = expression(paste("95% confidence intervals for ", p, 
                                sep = ""))) +
  scale_color_manual(values = c("blue", "orange")) 
```

We see that:

* In 31 cases, the confidence intervals "capture" the true $p = 900 / 2400 = 0.375$
* In 2 cases, the confidence intervals do not "capture" the true $p = 900 / 2400 = 0.375$

Thus, the confidence intervals capture the true proportion $31 / 33$ = `r 31 / 33 * 100`% of the time using this theory-based methodology.

#### Confidence intervals based on 100 virtual samples {-}

Let's say however, we repeated the above 100 times, not tactilely, but virtually. Let's do this only 100 times instead of 1000 like we did before so that the results can fit on the screen. Again, the steps for compute a 95% confidence interval for $p$ are:

1. Collect a sample of size $n = 50$ as we did in Chapter \@ref(sampling)
1. Compute $\widehat{p}$: the sample proportion red of these $n$ = 50 balls
1. Compute the standard error $\text{SE} = \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}$
1. Compute the margin of error $\text{MoE} = 1.96 \cdot \text{SE} =  1.96 \cdot \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}$
1. Compute both end points of the confidence interval:
    + `lower_ci`: $\widehat{p} - \text{MoE} = \widehat{p} - 1.96 \cdot \text{SE} = \widehat{p} - 1.96 \cdot \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}$
    + `upper_ci`: $\widehat{p} + \text{MoE} = \widehat{p} + 1.96 \cdot \text{SE} = \widehat{p} +1.96 \cdot \sqrt{\frac{\widehat{p}(1-\widehat{p})}{n}}$

Run the following three steps, being sure to `View()` the resulting data frame after each step so you can convince yourself of what's going on:

```{r}
# First: Take 100 virtual samples of n=50 balls
virtual_samples <- bowl %>% 
  rep_sample_n(size = 50, reps = 100)

# Second: For each virtual sample compute the proportion red
virtual_prop_red <- virtual_samples %>% 
  group_by(replicate) %>% 
  summarize(red = sum(color == "red")) %>% 
  mutate(prop_red = red / 50)

# Third: Compute the 95% confidence interval as above
virtual_prop_red <- virtual_prop_red %>% 
  rename(p_hat = prop_red) %>% 
  mutate(
    n = 50,
    SE = sqrt(p_hat*(1-p_hat)/n),
    MoE = 1.96 * SE,
    lower_ci = p_hat - MoE,
    upper_ci = p_hat + MoE
  )
```

Here are the results:

```{r virtual-conf-int, echo=FALSE, message=FALSE, warning=FALSE, fig.height=6, fig.cap="100 confidence intervals based on 100 virtual samples of size n=50"}
set.seed(79)

virtual_samples <- bowl %>% 
  rep_sample_n(size = 50, reps = 100)

# Second: For each virtual sample compute the proportion red
virtual_prop_red <- virtual_samples %>% 
  group_by(replicate) %>% 
  summarize(red = sum(color == "red")) %>% 
  mutate(prop_red = red / 50)

# Third: Compute the 95% confidence interval as above
virtual_prop_red <- virtual_prop_red %>% 
  rename(p_hat = prop_red) %>% 
  mutate(
    n = 50,
    SE = sqrt(p_hat * (1 - p_hat) / n),
    MoE = 1.96 * SE,
    lower_ci = p_hat - MoE,
    upper_ci = p_hat + MoE
  ) %>% 
  mutate(
    y = seq_len(n()),
    p = 900 / 2400,
    captured = lower_ci <= p & p <= upper_ci
  )

ggplot(virtual_prop_red) +
  geom_point(aes(x = p_hat, y = y, color = captured)) +
  geom_segment(aes(y = y, yend = y, x = lower_ci, xend = upper_ci, 
                   color = captured)) +
  labs(
    x = expression("Proportion red"),
    y = "Replicate ID",
    title = expression(paste("95% confidence intervals for ", p, sep = ""))
  ) +
  scale_color_manual(values = c("blue", "orange")) + 
  geom_vline(xintercept = 900 / 2400, color = "red") 
```

We see that of our 100 confidence intervals based on samples of size $n$ = 50, `r sum(virtual_prop_red[["captured"]])` of them captured the true $p = 900/2400$, whereas `r 100 - sum(virtual_prop_red[["captured"]])` of them missed. As we create more and more confidence intervals based on more and more samples, about 95% of these intervals will capture. In other words our procedure is "95% reliable." 

Theoretical methods like this have largely been used in the past since we didn't have the computing power to perform simulation-based methods such as bootstrapping. They are still commonly used though and if the normality assumptions are met, they can provide a nice option for finding confidence intervals and performing hypothesis tests as we will see in Chapter \@ref(hypothesis-testing).

#### Where does the 1.96 come from? {-}

We've been mentioning quite a bit throughout this chapter that if the distributions are bell-shaped and symmetric that things will likely work nicely for us. This bell-shaped distribution is commonly called the Gaussian or normal distribution. It has that characteristic shape of a bell that we've discussed.

The 1.96 in our formula for a 95% theory-based confidence interval is directly related to the normal distribution. The normal distribution is actually a family of distributions with each characterized by their mean and their standard deviation. The _standard normal distribution_ is one of the most common since it acts as a standardization for all of the other normal distributions. In other words, via some formulas, any value of a normal distribution can be converted to its corresponding value on the standard normal distribution. Let's take a look visually at this standard normal distribution and the range of different values it can take on.


```{r std-normal-setup, echo=FALSE}
ggplot(data = data.frame(x = c(-3.5, 3.5)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) + 
  scale_x_continuous(breaks = seq(-3.5, 3.5, 0.5)) +
  ylab("") +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank())
```

Let's draw a vertical line at both 1.96 and -1.96 on this plot.

```{r std-normal, echo=FALSE}
ggplot(data = data.frame(x = c(-3.5, 3.5)), aes(x)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1)) + 
  scale_x_continuous(breaks = seq(-3.5, 3.5, 0.5)) +
  ylab("") +
  theme(axis.title.y = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks.y = element_blank()) +
  geom_vline(xintercept = 1.96, color = "green", size = 2) +
  geom_vline(xintercept = -1.96, color = "green", size = 2)
```

Any guesses to how much area is under the black curve and to the right 1.96? The correct answer is very close to 2.5%. Since the normal distribution is symmetric there is also 2.5% of the area to the left of -1.96. Therefore, if we wanted to encapsulate the middle 95% of the values on the standard normal distribution we'd be pretty close to between -1.96 and 1.96. That's the reason why we choose 1.96 as our multiplier in the formula above.

What if we wanted to get the multiplier for say a 90% theory-based confidence interval? R has a built-in function to help us with that:

```{r}
qnorm(p = 0.95)
```

Here `q` stands for "quantile" and `norm` stands for normal. So the 95^th^ percentile of the standard normal distribution falls at around 1.65. Let's check to see where the 2.5^th^ percentile falls:

```{r}
qnorm(p = 0.025)
```

This is what we expected above. Close to -1.96 corresponds to the spot that is 2.5% of the way into the values of the standard normal distribution. We'll elaborate more on these theory-based methods in Chapter \@ref(hypothesis-testing), but this should give you a good start!

---


```{block, type='learncheck', purl=FALSE}
**_Learning check_**
```

Practice problems to come soon!

```{block, type='learncheck', purl=FALSE}
```



### Summary table {#ci-conclusion-table}

In this chapter, we performed both tactile and virtual simulations of resampling/bootstrapping to infer about unknown parameters. We also presented a case study of bootstrapping in a real-life situation: the suggested contagiousness of yawning. We used the sample proportion $\widehat{p}$ to estimate the population proportion $p$ and the sample mean $\overline{x} = \widehat{\mu}$ to estimate the population mean. We also explored a two variable problem in our yawning case study. Let's review these and others again in Table \@ref(tab:summarytable-ch9). 

```{r summarytable-ch9, echo=FALSE, message=FALSE}
# The following Google Doc is published to CSV and loaded below using read_csv() below:
# https://docs.google.com/spreadsheets/d/1QkOpnBGqOXGyJjwqx1T2O5G5D72wWGfWlPyufOgtkk4/edit#gid=0

"https://docs.google.com/spreadsheets/d/e/2PACX-1vRd6bBgNwM3z-AJ7o4gZOiPAdPfbTp_V15HVHRmOH5Fc9w62yaG-fEKtjNUD2wOSa5IJkrDMaEBjRnA/pub?gid=0&single=true&output=csv" %>% 
  read_csv(na = "") %>% 
  kable(
    caption = "\\label{tab:summarytable-ch9}Scenarios of sampling for inference", 
    booktabs = TRUE,
    escape = FALSE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position")) %>%
  column_spec(1, width = "0.5in") %>% 
  column_spec(2, width = "0.7in") %>%
  column_spec(3, width = "1in") %>%
  column_spec(4, width = "1.1in") %>% 
  column_spec(5, width = "1in")
```

We'll cover all the remaining scenarios as follows, using the terminology, notation, and definitions related to sampling you saw in Section \@ref(sampling-framework):

* In Chapter \@ref(hypothesis-testing), we'll see an example of statistical inference for
    + Scenario 4: The difference $\mu_1 - \mu_2$ in average IMDB ratings for action and romance movies. This is another example of *two-sample* inference.
* In Chapter \@ref(inference-for-regression), we'll cover an example of statistical inference for the relationship between teaching score and various instructor demographic variables you saw in Chapter \@ref(regression) on basic regression and Chapter \@ref(multiple-regression) on multiple regression. Specifically
    + Scenario 5: The intercept $\beta_0$ of some population regression line.
    + Scenario 6: The slope $\beta_1$ of some population regression line.

### Additional resources

An R script file of all R code used in this chapter is available [here](scripts/09-confidence-intervals.R).

### What's to come?

This chapter introduced the notions of bootstrapping and confidence intervals as ways to build intuition about population parameters using only the original sample information. We also concluded with a glimpse into statistical significance and we'll dig much further into this in Chapter \@ref(hypothesis-testing) up next!

  
