# Multiple Regression {#multiple-regression}

```{r, include=FALSE, purl=FALSE}
chap <- 7
lc <- 0
rq <- 0
# **`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`**
# **`r paste0("(RQ", chap, ".", (rq <- rq + 1), ")")`**

knitr::opts_chunk$set(
  tidy = FALSE, 
  out.width = '\\textwidth', 
  fig.height = 4,
  fig.align='center',
  warning = FALSE
)

options(scipen = 99, digits = 3)

# In knitr::kable printing replace all NA's with blanks
options(knitr.kable.NA = '')

# Set random number generator see value for replicable pseudorandomness. Why 76?
# https://www.youtube.com/watch?v=xjJ7FheCkCU
set.seed(76)
```

In Chapter \@ref(regression) we introduced ideas related to modeling for explanation, in particular that the goal of modeling is make explicit the relationship between some outcome variable $y$ and some explanatory variable $x$. While there are many approaches to modeling, we focused on one particular technique: *linear regression*, one of the most commonly-used and easy-to-understand approaches to modeling. Furthemore to keep things simple we only considered models with one explanatory $x$ variable that was either numerical in Section \@ref(model1) or categorical in Section \@ref(model2).

In this chapter on multiple regression we'll start considering models that include more than one explanatory variable $x$. You can imagine when trying to model a particular outcome variable, like teaching evaluation scores as in Section \@ref(model1) or life expectancy as in Section \@ref(model2), that it would be very useful to include more than just one explanatory variable's worth of information. 

Since our regression models will now consider more than one explanatory variable, the interpretation of the associated effect of any one explanatory variable must be made in conjunction with the other explanatory variables included in your model. Let's begin!

### Needed packages {-}

Let's load all the packages needed for this chapter (this assumes you've already installed them). Recall from our discussion in Section \@ref(tidyverse-package) that loading the `tidyverse` package by running `library(tidyverse)` loads the following commonly used data science packages all at once:

* `ggplot2` for data visualization
* `dplyr` for data wrangling
* `tidyr` for converting data to "tidy" format
* `readr` for importing spreadsheet data into R
* As well as the more advanced `purrr`, `tibble`, `stringr`, and `forcats` packages

If needed, read Section \@ref(packages) for information on how to install and load R packages. 

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(moderndive)
library(skimr)
library(ISLR)
```

```{r, message=FALSE, warning=FALSE, echo=FALSE}
# Packages needed internally, but not in text:
library(kableExtra)
library(patchwork)
library(gapminder)
```



***



## One numerical & one categorical explanatory variable {#model4}

Let's revisit the instructor evaluation data we introduced in Section \@ref(model1), where we studied the relationship between instructor evaluation scores (as given by students) and their "beauty" scores for instructors teaching courses at the UT Austin; the variable teaching `score` was a numerical outcome variable $y$ and the variable beauty score `bty_avg` was a numerical explanatory $x$ variable. 

In this section we are going to consider a different model. Our outcome variable will still be teaching score, but now including two different explanatory variables: age and gender. Could it be that instructors who are older receive better teaching evaluations from students? Or could it instead be that younger instructors receive better evaluations? Are there differences in evaluations given by students for instructors of different genders? We'll answer these questions by modeling the relationship between these variables using *multiple regression* where we have:

1. A numerical outcome variable $y$, as before the instructor's teaching score and
1. Two explanatory variables:
    1. A numerical explanatory variable $x_1$, the instructor's age
    1. A categorical explanatory variable $x_2$, the instructor's binary gender (male or female).

It is important to note that at the time of this study, due to then commonly held beliefs about gender, this variable was often recorded as a binary. While the results of a model that oversimplies gender this way may be imperfect, we still found the results to be very pertinent and relevant today. An eminent statistician by the name George E.P. Box summarizes our thinking very nicely: ["All models are wrong, but some are useful."](https://en.wikipedia.org/wiki/All_models_are_wrong).



### Exploratory data analysis {#model4EDA}

The data on the 463 courses at the UT Austin can be found in the `evals` data frame included in the `moderndive` package. However, to keep things simple, let's `select()` only the subset of the variables we'll consider in this chapter, and save this data in a new data frame called `eval_ch7`. Note that these are different than the variables chosen in Chapter 6. 

```{r}
evals_ch7 <- evals %>%
  select(ID, score, age, gender)
```

Recall the three common steps in an exploratory data analysis we saw in Section \@ref(model1EDA)

1.  Looking at the raw data values.
1. Computing summary statistics, like means, medians, and interquartile ranges.
1. Creating data visualizations.

Let's first look at the raw data values both either looking at `evals_ch7` RStudio's spreadsheet viewer or using the `glimpse()` function

```{r}
glimpse(evals_ch7)
```

Let's also display a random sample of 5 rows of the 463 rows corresponding to different courses in Table \@ref(tab:model4-data-preview). Remember due to the random nature of the sampling, you will likely end up with a different subset of 5 rows.

```{r, eval=FALSE}
evals_ch7 %>%
  sample_n(size = 5)
```
```{r model4-data-preview, echo=FALSE}
evals_ch7 %>%
  sample_n(5) %>%
  knitr::kable(
    digits = 3,
    caption = "A random sample of 5 out of the 463 courses at UT Austin",
    booktabs = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position"))
```

Now that we've looked at the raw values in our `evals_ch7` data frame and obtained a sense of the data, let's move on to next common step in an exploratory data analysis: computing summary statistics. As we did in our exploratory data analyses in Sections \@ref(model1EDA) and  \@ref(model2EDA) from the previous chapter, let's use the `skim()` function from the `skimr` package, being sure to only `select()` the columns of interest:

```{r}
evals_ch7 %>% 
  select(score, age, gender) %>% 
  skim()
```

Observe for example that we have no missing data, courses taught by 268 male vs 195 female instructors, and and average age of 48.37. Recall however that each row in our data represents a particular course and that instructors can teach more than one course. Therefore the average age of the unique instructors may differ. 

Furthermore, let's compute the correlation between our two numerical variables: `score` and `age`. Recall from Section \@ref(model1EDA) that correlation coefficients only exist between numerical variables. We observe that they are weakly negatively correlated.

```{r}
evals_ch7 %>% 
  get_correlation(formula = score ~ age)
```

Let's now perform the last of the three common steps in an exploratory data analysis: creating data visualizations. Given that the outcome variable `score` and explanatory variable `age` are both numerical, we'll use a scatterplot to display their relationship. How can we incorporate the categorical variable `gender` however? By mapping the variable `gender` to the color aesthetic and creating a *colored* scatterplot! The following code is very similar to the code that created the scatterplot of teaching score and beauty score in Figure \@ref(fig:numxplot1), but with `color = gender` added to the `aes()`.

```{r numxcatxplot1, warning=FALSE, fig.cap="Colored scatterplot of relationship of teaching and beauty scores"}
ggplot(evals_ch7, aes(x = age, y = score, color = gender)) +
  geom_point() +
  labs(x = "Age", y = "Teaching Score", color = "Gender") +
  geom_smooth(method = "lm", se = FALSE)
```

In the resulting Figure \@ref(fig:numxcatxplot1), observe that `ggplot` assigns a default red/blue color scheme to the points and lines associated with each of the two levels of `gender`: `female` and `male`. Furthermore the `geom_smooth(method = "lm", se = FALSE)` layer automatically fits a different regression line for each group since we have provided `color = gender` in the aesthetic mapping. This allows for all subsequent geometries to have the same aesthetic mappings.

We notice some interesting trends:

1. There are almost no women faculty over the age of 60 as evidenced by lack of red dots above $x$ = 60.
1. While both regression lines are negatively sloped with age (i.e. as instructor's age, so also do they tend to receive lower teaching scores), the slope for age for the female instructors is *more* negative. In other words, the female instructors are paying a harsher penalty in their teaching scores then the male instructors do. 

### Interaction model {#model4interactiontable}
  
Let's now quantify the relationship of our outcome variable $y$ and two explanatory variables using one type of multiple regression model known as an "interaction model." Unfortunately, we don't have enough context at this point to explain where the term "interaction" comes from; we'll explain why statisticians use this term at the end of this section.

In particular, we'll write out the equation of the two regression lines in Figure  \@ref(fig:numxcatxplot1) using the values from a regression table. Before we do this however, let's go over a brief refresher of regression when you have a categorical explanatory variable $x$. 

Recall in Section \@ref(model2table) we fit a regression model for countries' life expectancy as a function of which continent the country was in. In other words we had a numerical outcome variable $y$ = `lifeExp` and a categorical explanatory variable $x$ = `continent` which had 5 levels: `Africa`, `Americas`, `Asia`, `Europe`, and `Oceania`. Let's redisplay the regression table you saw in Table \@ref(tab:catxplot4b):

```{r, echo=FALSE}
# Wrangle data
gapminder2007 <- gapminder %>%
  filter(year == 2007) %>%
  select(country, lifeExp, continent, gdpPercap)

# Fit regression model:
lifeExp_model <- lm(lifeExp ~ continent, data = gapminder2007)

# Get regression table and kable output
get_regression_table(lifeExp_model) %>%
  knitr::kable(
    digits = 3,
    caption = "Regression table for life expectancy as a function of continent.",
    booktabs = TRUE
  ) %>%
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position"))
```

Recall our interpretations of the `estimate` column. Since `Africa` was the "baseline for comparison" group since Africa comes first alphabetically, the `intercept` term corresponds to the mean life expectancy for all countries in Africa of 54.8 years. The other 4 values of `estimate` correspond to "offsets" relative to the baseline group. So for example, the "offset" corresponding to the Americas is +18.8 versus the baseline for comparison group Africa i.e. the average life expectancy for countries in the Americas is 18.8 years *higher*. Thus the mean life expectancy for all countries in the Americas is 54.8 + 18.8 = 73.6. The same interpretation holds for Asia, Europe, and Oceania.

Going to back to our multiple regression model for teaching `score` using `age` and `gender` in Figure \@ref(fig:numxcatxplot1), we generate the regression table using the same two step approach from Chapter \@ref(regression): we first "fit" the model using the `lm()` "linear model" function and then we apply the `get_regression_table()` function. This time however our model formula won't be of form `y ~ x`, but rather of form `y ~ x1 * x2`. In other words our two explanatory variables `x1` and `x2` are separated by a `*` sign:

```{r, eval=FALSE}
# Fit regression model:
score_model_interaction <- lm(score ~ age * gender, data = evals_ch7)
# Get regression table:
get_regression_table(score_model_interaction)
```
```{r regtable-interaction, echo=FALSE}
score_model_interaction <- lm(score ~ age * gender, data = evals_ch7)
get_regression_table(score_model_interaction) %>% 
  knitr::kable(
    digits = 3,
    caption = "Regression table for interaction model.", 
    booktabs = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position"))
```

Looking the regression table output in Table \@ref(tab:regtable-interaction), we see there are four rows of values in the `estimate` column. While it is not immediately apparent, using these four values we can write out the equations of both the red and blue lines in Figure \@ref(fig:numxcatxplot1). Let's build these up.

First, since the word `female` is alphabetically before `male`, female instructors are the "baseline for comparison" group. Therefore `intercept` is the intercept and `age` is the slope for age *for only the female instructors*. In other words, the red reression line in Figure \@ref(fig:numxcatxplot1) has intercept 4.883 and slope for age of -0.018. Remember that for this particular data, while the intercept has a mathematical interpretation, it has no *practical* interpretation since there can't be any instructors with age = 0.

What about the intercept and slope for age of the male instructors? In other words the blue line in Figure \@ref(fig:numxcatxplot1)? This is where our notion of "offsets" comes into play once again. The value for `gendermale` of -0.446 is not the intercept for the male instructors, but rather the *offset* (or difference) in intercept for male instructors relative to female instructors. Therefore, the intercept for the male instructors is `intercept + gendermale` = 4.883 + (-0.446) = 4.883 - 0.446 = 4.437. 

Similarly, `age:gendermale` = 0.014 is not the slope for age for the male instructors, but rather the *offset* (or difference) in slope for the male instructors. Therefore, the slope for age for the male instructors is `age + age:gendermale` =  -0.018 + 0.014 = -0.004. Therfore the blue reression line in Figure \@ref(fig:numxcatxplot1) has intercept 4.437 and slope for age of -0.004.

Let's summarize these values in Table \@ref(tab:interaction-summary) and focus on the two slopes for age:

```{r interaction-summary, echo=FALSE}
tibble(
  Gender = c("Female instructors", "Male instructors"),
  Intercept = c(4.883, 4.437),
  `Slope for age` = c(-0.018, -0.004)
) %>% 
  knitr::kable(
    digits = 3,
    caption = "Comparison of female and male intercepts and age slopes", 
    booktabs = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position"))
```

Since the slope for age for the female instructors was -0.018, it means that for every additional year in age for female instructors, there is an associated *decrease* of on average 0.018 units in teaching score. For the male instructors however, the corresponding associated decrease was on average only 0.004 units. While both slopes for age were negative, the slope for age for the female instructors is *more negative*. This is consistent with our observation from Figure \@ref(fig:numxcatxplot1), that this model is suggesting female instructors are paying a heavier price for aging in the evaluations they receive from students.

Let's now write the equation for our regression lines, which we can use to compute our fitted values $\widehat{y} = \widehat{\text{score}}$.

$$
\begin{align}
\widehat{y} = \widehat{\text{score}} &= b_0 + b_{\mbox{age}} \cdot \mbox{age} + b_{\mbox{male}} \cdot \mathbb{1}_{\mbox{is male}}(x) + b_{\mbox{age,male}} \cdot \mbox{age} \cdot \mathbb{1}_{\mbox{is male}}\\
&= 4.883 -0.018	\cdot \mbox{age} - 0.446 \cdot \mathbb{1}_{\mbox{is male}}(x) + 0.014 \cdot \mbox{age} \cdot \mathbb{1}_{\mbox{is male}}
\end{align}
$$

Whoa! That's even more daunting than the equation you saw for the life expectancy as a function of continent in Section \@ref(model2table)! However if you recall what an "indicator function" AKA "dummy variable" does, the equation simplifies greatly. In the above equation, we have one indicator function of interest:

$$
\mathbb{1}_{\mbox{is male}}(x) = \left\{
\begin{array}{ll}
1 & \text{if } \text{instructor } x \text{ is male} \\
0 & \text{otherwise}\end{array}
\right.
$$

Second, let's match coefficients in the above equation with values in the `estimate` column in our regression table in Table \@ref(tab:regtable-interaction):

1. $b_0$ is the `intercept` = 4.883 *for the female instructors*
1. $b_{\mbox{age}}$ is the slope for `age` = -0.018 *for the female instructors*
1. $b_{\mbox{male}}$ is the *offset in intercept for the male instructors*
1. $b_{\mbox{age,male}}$ is the *offset in slope for age for the male instructors*

Let's put this all together and compute the fitted value $\widehat{y} = \widehat{\text{score}}$ for female instructors. Since for female instructors $\mathbb{1}_{\mbox{is male}}(x)$ = 0, the above equation becomes

$$
\begin{align}
\widehat{y} = \widehat{\text{score}} &= b_0 + b_{\mbox{age}} \cdot \mbox{age} + b_{\mbox{male}} \cdot \mathbb{1}_{\mbox{is male}}(x) + b_{\mbox{age,male}} \cdot \mbox{age} \cdot \mathbb{1}_{\mbox{is male}}\\
&= 4.883 - 0.018	\cdot \mbox{age} - 0.446 \cdot \mathbb{1}_{\mbox{is male}}(x) + 0.014 \cdot \mbox{age} \cdot \mathbb{1}_{\mbox{is male}}\\
&= 4.883 - 0.018	\cdot \mbox{age} - 0.446 \cdot 0 + 0.014 \cdot \mbox{age} \cdot 0\\
&= 4.883 - 0.018	\cdot \mbox{age} - 0 + 0\\
&= 4.883 - 0.018	\cdot \mbox{age}\\
\end{align}
$$

which is the equation of the red regression line in Figure \@ref(fig:numxcatxplot1) corresponding to the female instructors. Correspondingly, since for male instructors $\mathbb{1}_{\mbox{is male}}(x)$ = 1, the above equation becomes

$$
\begin{align}
\widehat{y} = \widehat{\text{score}} &= 4.883 - 0.018	\cdot \mbox{age} - 0.446 \cdot \mathbb{1}_{\mbox{is male}}(x) + 0.014 \cdot \mbox{age} \cdot \mathbb{1}_{\mbox{is male}}\\
&= 4.883 - 0.018	\cdot \mbox{age} - 0.446 \cdot 1 + 0.014 \cdot \mbox{age} \cdot 1\\
&= 4.883 - 0.018	\cdot \mbox{age} - 0.446 + 0.014 \cdot \mbox{age}\\
&= (4.883 - 0.446) + (- 0.018 + 0.014) * \mbox{age}\\
&= 4.437 - 0.004	\cdot \mbox{age}\\
\end{align}
$$

which is the equation of the blue regression line in Figure \@ref(fig:numxcatxplot1) corresponding to the male instructors. 

Phew! That was a lot of arithmetic! Don't fret however, this is as hard as modeling will get in this book. If you're still a little unsure about using indicator functions and using categorical explanatory variables, we *highly* suggest you re-read Section \@ref(model2table) which involves only a single categorical explanatory variable and thus is much simpler. 

Before we end this section, we explain why we refer to this type of model as an "interaction model." The $b_{\mbox{age,male}}$ term in the equation for the fitted value $\widehat{y}$ = $\widehat{\text{score}}$ is what's known in statistical modeling as an "interaction effect." The interaction term corresponds to the `age:gendermale` = 0.014 in the final row of the regression table in Table \@ref(tab:regtable-interaction).  

We say there is an interaction effect if the associated effect of one variable *depends on the value of another variable*, in other words the two variables are "interacting." In our case, the associated effect of the variable age *depends* on the value of another variable, gender. This was evidenced by the difference in slopes for age of +0.014 of male instructors relative to female instructors. 

Another way of thinking of interaction effects is as follows. For a given instructor at the UT Austin, there might be an associated effect of their age on their teaching scores, there might be an associated effect of the gender on their teaching scores, but when put together, there might an *additional effect due to the intersection* of their age and their gender. 


### Parallel slopes model {#model4table}

When creating regression models with one numerical and one categorical explanatory variable, we are not just limited to interaction models as we just saw. Another type of model we can use is known as the "parallel slopes" model. Unlike with interaction models where the regression line can have both different intercepts and different slopes, parallel slopes models still allow for different intercetps but *force* all lines to have the same slope. The resulting regression lines are thus parallel. Let's visualize the best fitting parallel slopes model to our `evals_ch7` data.

Unfortunately, the `ggplot2` package does not have a convenient way to plot a parallel slopes model. We therefore created our own function `gg_parallel_slopes()` and included it in the `moderndive` package:

```{r numxcatx-parallel, warning=FALSE, fig.cap="Parallel slopes model of relationship of score with age & gender."}
gg_parallel_slopes(y = "score", num_x = "age", cat_x = "gender", data = evals_ch7)
```

Note the arguments i.e. inputs to this function: the outcome variable `y = "score"`, the numerical explanatory variable `num_x = "age"`, the categorical explanatory variable `cat_x = "gender"`, and the data frame that includes this `data = evals_ch7`. Be careful to include the quotation marks when specifying all variables, something you don't have to do when creating a visualization with `ggplot()`.

Observe in Figure \@ref(fig:numxcatx-parallel) that we now have parallel red and blue lines corresponding to the female and male instructors respectively, in other words they have the same negative slope. In other words, as instructors age, so also do they tend to receive lower teaching evaluation scores from students. However these two lines have different intercepts as evidenced by the fact that the blue line corresponding to the male instructors is higher than the red line corresponding to the female instructors. 

In order to obtain the precise numerical values of the intercepts and the common slope, we once again first "fit" the model using the `lm()` "linear model" function and then we apply the `get_regression_table()` function. However, unlike the interaction model which had a model formula of form `y ~ x1 * x2`, our model formula is now of form `y ~ x1 + x2`. In other words our two explanatory variables `x1` and `x2` are separated by a `+` sign:

```{r, eval=FALSE}
# Fit regression model:
score_model_interaction <- lm(score ~ age * gender, data = evals_ch7)
# Get regression table:
get_regression_table(score_model_interaction)
```
```{r regtable-parallel-slopes, echo=FALSE}
score_model_parallel_slopes <- lm(score ~ age + gender, data = evals_ch7)
get_regression_table(score_model_parallel_slopes) %>% 
  knitr::kable(
    digits = 3,
    caption = "Regression table for parallel slopes model.", 
    booktabs = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position"))
```

Similarly to the regression table for the interaction model from our earlier Table \@ref(tab:regtable-interaction), we have an `intercept` term corresponding to the intercept for the "baseline for comparison" female instructor group and a `gendermale` term corresponding to the *offset* (or difference) in intercept for the male instructors relative to female instructors. In other words in Figure \@ref(fig:regtable-parallel-slopes) the red regression line corresponding to the female instructors has an intercept of 4.484 while the blue regression line corresponding to the male instructors has an intercept of 4.484 + 0.191 = 4.67. Once again, since there aren't any instructors of age 0, the intercepts only have a mathematical intepretation but no practical one. 

Unlike in Table \@ref(tab:regtable-interaction) we now only have a single term relating to the slope for age as we've forced both the female and male instructors to have a common slope for age of -0.009. In other words, for every increase of 1 year in instructor age, we observe an associated decrease of on average 0.009 units in teaching for *both* the female and male instructor. 

Let's now write the equation for our regression lines, which we can use to compute our fitted values $\widehat{y} = \widehat{\text{score}}$.

$$
\begin{align}
\widehat{y} = \widehat{\text{score}} &= b_0 + b_{\mbox{age}} \cdot \mbox{age} + b_{\mbox{male}} \cdot \mathbb{1}_{\mbox{is male}}(x)\\
&= 4.484 -0.009	\cdot \mbox{age} + 0.191 \cdot \mathbb{1}_{\mbox{is male}}(x) 
\end{align}
$$

Let's put this all together and compute the fitted value $\widehat{y} = \widehat{\text{score}}$ for female instructors. Since for female instructors $\mathbb{1}_{\mbox{is male}}(x)$ = 0, the above equation becomes

$$
\begin{align}
\widehat{y} = \widehat{\text{score}} &= b_0 + b_{\mbox{age}} \cdot \mbox{age} + b_{\mbox{male}} \cdot \mathbb{1}_{\mbox{is male}}(x)\\
&= 4.484 -0.009	\cdot \mbox{age} + 0.191 \cdot \mathbb{1}_{\mbox{is male}}(x)\\
&= 4.484 -0.009	\cdot \mbox{age} + 0.191 \cdot 0\\
&= 4.484 -0.009	\cdot \mbox{age}
\end{align}
$$

which is the equation of the red regression line in Figure \@ref(fig:numxcatx-parallel) corresponding to the female instructors. Correspondingly, since for male instructors $\mathbb{1}_{\mbox{is male}}(x)$ = 1, the above equation becomes

$$
\begin{align}
\widehat{y} = \widehat{\text{score}} &= b_0 + b_{\mbox{age}} \cdot \mbox{age} + b_{\mbox{male}} \cdot \mathbb{1}_{\mbox{is male}}(x)\\
&= 4.484 -0.009	\cdot \mbox{age} + 0.191 \cdot \mathbb{1}_{\mbox{is male}}(x)\\
&= 4.484 -0.009	\cdot \mbox{age} + 0.191 \cdot 1\\
&= (4.484 + 0.191) - 0.009 \cdot \mbox{age}\\
&= 4.67 -0.009 \cdot \mbox{age}
\end{align}
$$

which is the equation of the blue regression line in Figure \@ref(fig:numxcatx-parallel) corresponding to the male instructors. 

Great! We've considered both an interaction model and a parallel slopes model for our data. Let's compare the visualizations for both models side-by-side in Figure \@ref(fig:numxcatx-comparison)

```{r numxcatx-comparison, fig.width = 8, echo = FALSE, warning=FALSE, fig.cap="Comparison of interaction and parallel slopes models."}
p1 <- ggplot(evals_ch7, aes(x = age, y = score, color = gender), show.legend = FALSE) +
  geom_point() +
  labs(x = "Age", y = "Teaching Score", title = "Interaction model") +
  geom_smooth(method = "lm", se = FALSE) +
  theme(legend.position = "none")
p2 <- gg_parallel_slopes(y = "score", num_x = "age", cat_x = "gender", data = evals_ch7) +
  labs(x = "Age", y = "Teaching Score", title = "Parallel slopes model") +
  theme(axis.title.y = element_blank())
p1 + p2
```

At this point, you might be asking yourself: "Why would we ever use an parallel slopes model?" Looking at the left-hand plot in Figure \@ref(fig:numxcatx-comparison), the two lines definitely do not appear to be parallel, so why would we *force* them to be parallel as in the right-hand plot?

The answer lies in a philosophical principle known as "Occam's Razor" which states that "all other things being equal, simpler solutions are more likely to be correct than complex ones." When viewed in a modeling framework, Occam's Razor can be recast as "all other things being equal, simpler models are to be preferred over complex ones." In other words, we should only favor the more complex model if the additional complexity is warranted. 

Let's compare the equations for the regression line for both the interaction and parallel slopes model:

$$
\begin{align}
\text{Interaction} &: \widehat{y} = \widehat{\text{score}} = b_0 + b_{\mbox{age}} \cdot \mbox{age} + b_{\mbox{male}} \cdot \mathbb{1}_{\mbox{is male}}(x) + b_{\mbox{age,male}} \cdot \mbox{age} \cdot \mathbb{1}_{\mbox{is male}}\\
\text{Parallel slopes} &: \widehat{y} = \widehat{\text{score}} = b_0 + b_{\mbox{age}} \cdot \mbox{age} + b_{\mbox{male}} \cdot \mathbb{1}_{\mbox{is male}}(x)
\end{align}
$$

The interaction model is "more complex" in that there is an additional $b_{\mbox{age,male}} \cdot \mbox{age} \cdot \mathbb{1}_{\mbox{is male}}$ element to the equation not present for the parallel slopes model. Furthermore we should only favor the interaction model over the parallel slopes model when this additional complexity is *warranted*.

So while in Figure \@ref(fig:numxcatx-comparison) it appears this additional complexity is warranted given the clearly different slopes in the interaction model, in Section \@ref(model-selection) below on model selection we'll present data where the case for using an interaction model isn't as strong.


### Observed/fitted values and residuals {#model4points}

For brevity's sake, in this section we'll only compute the observed values, fitted values, and residuals for the interaction model which we saved in `score_model_interaction`.

In Figure \@ref(fig:fitted-values)

```{r fitted-values, echo = FALSE, warning=FALSE, fig.cap="Fitted values for two new professors"}
newpoints <- evals_ch7 %>% 
  slice(c(1, 5)) %>% 
  get_regression_points(score_model_interaction, newdata = .)

ggplot(evals_ch7, aes(x = age, y = score, color = gender), show.legend = FALSE) +
  geom_point() +
  labs(x = "Age", y = "Teaching Score", title = "Interaction model") +
  geom_smooth(method = "lm", se = FALSE) +
  theme(legend.position = "none") +
  geom_vline(data = newpoints, aes(xintercept = age, col = gender), linetype = "dashed", size = 1) +
  geom_point(data = newpoints, aes(x= age, y = score_hat), size = 6)
```

FILL THIS IN.

```{r, eval=FALSE}
regression_points <- get_regression_points(score_model_interaction)
regression_points
```
```{r model4-points-table, echo=FALSE}
regression_points <- get_regression_points(score_model_interaction)
regression_points %>%
  slice(1:5) %>%
  knitr::kable(
    digits = 3,
    caption = "Regression points (first 5 rows of 463)",
    booktabs = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position"))
```

FILL THIS IN.

```{block, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Compute the observed values, fitted values, and residuals not for the interaction model as we just did, but rather for the parallel slopes model we saved in `score_model_interaction`.

```{block, type='learncheck', purl=FALSE}
```


***



## Two numerical explanatory variables {#model3}

Let's now attempt to identify factors that are associated with how much credit card debt an individual will have. The textbook [An Introduction to Statistical Learning with Applications in R](http://www-bcf.usc.edu/~gareth/ISL/) by Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani is an intermediate-level textbook on statistical and machine learning freely available [here](http://www-bcf.usc.edu/~gareth/ISL/ISLR%20Seventh%20Printing.pdf). It has an accompanying R package called `ISLR` with datasets that the authors use to demonstrate various machine learning methods. One dataset that is frequently used by the authors is the `Credit` dataset where predictions are made on the credit card balance held by $n = 400$ credit card holders. These predictions are based on information about them like income, credit limit, and education level. Note that this dataset is not based on actual individuals, it is a simulated dataset used for educational purposes. 

Since no information was provided as to who these $n$ = 400 individuals are and how they came to be included in this dataset, it will be hard to make any scientific claims based on this data. Recall our discussion from the previous chapter that correlation does not necessarily imply causation. That being said, we'll still use `Credit` to demonstrate multiple regression with:

1. A numerical outcome variable $y$, in this case credit card balance.
1. Two explanatory variables:
    1. A first numerical explanatory variable $x_1$. In this case, their credit limit.
    1. A second numerical explanatory variable $x_2$. In this case, their income (in thousands of dollars).

In the forthcoming Learning Checks, we'll consider a different scenario:

1. The same numerical outcome variable $y$: credit card balance.
1. Two new explanatory variables:
    1. A first numerical explanatory variable $x_1$: their credit rating.
    1. A second numerical explanatory variable $x_2$: their age.


### Exploratory data analysis {#model3EDA}

Let's load the `Credit` data and `select()` only the needed subset of variables. 

```{r, warning=FALSE, message=FALSE}
library(ISLR)
credit <- Credit %>%
  as_tibble() %>% 
  select(ID, debt = Balance, credit_limit = Limit, income = Income, credit_rating = Rating, age = Age)
```

Let's look at the raw data values both by bringing up RStudio's spreadsheet viewer and the `glimpse()` function. Although in Table \@ref(tab:model3-data-preview) we only show 5 randomly selected credit card holders out of `r nrow(credit)`:

```{r, eval=FALSE}
View(credit)
```

```{r model3-data-preview, echo=FALSE}
credit %>%
  sample_n(5) %>%
  knitr::kable(
    digits = 3,
    caption = "Random sample of 5 credit card holders",
    booktabs = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position"))
```

```{r}
glimpse(credit)
```

Let's look at some summary statistics, again using the `skim()` function from the `skimr` package:

```{r}
credit %>% 
  select(debt, credit_limit, income) %>% 
  skim()
```


We observe for example:

1. The mean and median credit card balance are \$520.01 and \$459.50 respectively.
1. 25% of card holders had debts of $68.75 or less.
1. The mean and median credit card limit are \$4735.6 and \$4622.50 respectively.
1. 75% of these card holders had incomes of $57,470 or less.

Since our outcome variable `debt` and the explanatory variables `credit_limit` and
`income` are numerical, we can compute the correlation coefficient between pairs
of these variables. First, we could run the `get_correlation()` command as seen
in Subsection \@ref(model1EDA) twice, once for each explanatory variable:

```{r, eval=FALSE}
credit %>% 
  get_correlation(debt ~ credit_limit)
credit %>% 
  get_correlation(debt ~ income)
```

Or we can simultaneously compute them by returning a *correlation matrix* in 
Table \@ref(tab:model3-correlation). We can read off the correlation coefficient
for any pair of variables by looking them up in the appropriate row/column combination.

```{r, eval=FALSE}
credit %>%
  select(debt, credit_limit, income) %>% 
  cor()
```
```{r model3-correlation, echo=FALSE}
credit %>% 
  select(debt, credit_limit, income) %>% 
  cor() %>% 
  knitr::kable(
    digits = 3,
    caption = "Correlations between credit card balance, credit limit, and income", 
    booktabs = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position"))
```

For example, the correlation coefficient of:

1. `debt` with itself is 1 as we would expect based on the definition of the correlation coefficient.
1. `debt` with `credit_limit` is 0.862. This indicates a strong positive linear relationship, which makes sense as only individuals with large credit limits can accrue large credit card balances.
1. `debt` with `income` is 0.464. This is suggestive of another positive linear relationship, although not as strong as the relationship between `debt` and `credit_limit`.
1. As an added bonus, we can read off the correlation coefficient of the two explanatory variables, `credit_limit` and `income` of 0.792. In this case, we say there is a high degree of *collinearity* between these two explanatory variables. 

Collinearity (or multicollinearity) is a phenomenon in which one explanatory variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. So in this case, if we knew someone's credit card `credit_limit` and since `credit_limit` and `income` are highly correlated, we could make a fairly accurate guess as to that person's `income`. Or put loosely, these two variables provided redundant information. For now let's ignore any issues related to collinearity and press on.

Let's visualize the relationship of the outcome variable with each of the two explanatory variables in two separate plots:

```{r, eval=FALSE}
ggplot(credit, aes(x = credit_limit, y = debt)) +
  geom_point() +
  labs(x = "Credit limit (in $)", y = "Credit card balance (in $)", 
       title = "Relationship between balance and credit limit") +
  geom_smooth(method = "lm", se = FALSE)
  
ggplot(credit, aes(x = income, y = debt)) +
  geom_point() +
  labs(x = "Income (in $1000)", y = "Credit card balance (in $)", 
       title = "Relationship between balance and income") +
  geom_smooth(method = "lm", se = FALSE)
```

```{r 2numxplot1, echo=FALSE, fig.height=4, fig.cap="Relationship between credit card balance and credit limit/income"}
model3_balance_vs_limit_plot <- ggplot(credit, aes(x = credit_limit, y = debt)) +
  geom_point() +
  labs(x = "Credit limit (in $)", y = "Credit card balance (in $)", 
       title = "debt vs credit limit") +
  geom_smooth(method = "lm", se = FALSE)
model3_balance_vs_income_plot <- ggplot(credit, aes(x = income, y = debt)) +
  geom_point() +
  labs(x = "Income (in $1000)", y = "credit card balance (in $)", 
       title = "debt vs income") +
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_continuous(limits = c(0, NA))
model3_balance_vs_limit_plot + model3_balance_vs_income_plot
```

First, there is a positive relationship between credit limit and balance, since as credit limit increases so also does credit card balance; this is to be expected given the strongly positive correlation coefficient of 0.862. In the case of income, the positive relationship doesn't appear as strong, given the weakly positive correlation coefficient of 0.464. However the two plots in Figure \@ref(fig:2numxplot1) only focus on the relationship of the outcome variable with each of the explanatory variables independently. To get a sense of the *joint* relationship of all three variables simultaneously through a visualization, let's display the data in a 3-dimensional (3D) scatterplot, where

1. The numerical outcome variable $y$ `debt` is on the z-axis (vertical axis)
1. The two numerical explanatory variables form the "floor" axes. In this case
    1. The first numerical explanatory variable $x_1$ `income` is on of the floor axes.
    1. The second numerical explanatory variable $x_2$ `credit_limit` is on the other floor axis.

Click on the following image to open an interactive 3D scatterplot in your browser:

```{r, echo=FALSE, results='asis', purl=FALSE}
image_link(path = "images/credit_card_balance_3D_scatterplot.png", link = "https://assets.datacamp.com/production/repositories/1575/datasets/f369dc94041e88effd5ed66512978f8cdfd33801/03-01-slides-interactive_3D_scatterplot_regression_plane.html", html_opts = "width=200%", latex_opts = "width=0.6\\textwidth", alt_text = "3D scatterplot")
```

```{r, eval=FALSE, echo=FALSE}
# Save as 798 x 562 images/credit_card_balance_3D_scatterplot.png
library(ISLR)
library(plotly)
plot_ly(showscale=FALSE) %>%
  add_markers(
    x = credit$income,
    y = credit$credit_limit,
    z = credit$debt,
    hoverinfo = 'text',
    text = ~paste("x1 - Income: ", credit$income, 
                  "</br> x2 - Credit Limit: ", credit$credit_limit, 
                  "</br> y - Debt: ", credit$debt)
  ) %>% 
  layout(
    scene = list(
      xaxis = list(title = "x1 - Income (in $10K)"),
      yaxis = list(title = "x2 - Credit Limit ($)"),
      zaxis = list(title = "y - debt ($)")
    )
  )
```

Previously in Figure \@ref(fig:numxplot4), we plotted a "best-fitting" regression line through a set of points where the numerical outcome variable $y$ was teaching `score` and a single numerical explanatory variable $x$ was `bty_avg`. What is the analogous concept when we have *two* numerical predictor variables? Instead of a best-fitting line, we now have a best-fitting *plane*, which is a 3D generalization of lines which exist in 2D. Click [here](https://beta.rstudioconnect.com/connect/#/apps/3214/) to open an interactive plot of the regression plane shown below in your browser. Move the image around, zoom in, and think about how this plane generalizes the concept of a linear regression line to three dimensions.

<!-- Need to replace link here since RStudio Connect is Amherst? -->

```{r echo=FALSE, fig.cap="Regression plane", fig.align='center'}
knitr::include_graphics("images/credit_card_balance_regression_plane.png")
```

<!--
<center>
\begin{center}
`r image_link(path = "images/credit_card_balance_regression_plane.png", link = "https://beta.rstudioconnect.com/connect/#/apps/3214/", alt_text = "Regression plane")`
\end{center}
</center>
-->

```{r, eval=FALSE, echo=FALSE}
# Save as 798 x 562 images/credit_card_balance_regression_plane.png
library(ISLR)
library(plotly)
library(tidyverse)

# setup hideous grid required by plotly
model_lm <- lm(debt ~ income + credit_limit, data=credit)
x_grid <- seq(from=min(credit$income), to=max(credit$income), length=100)
y_grid <- seq(from=min(credit$credit_limit), to=max(credit$credit_limit), length=200)
z_grid <- expand.grid(x_grid, y_grid) %>%
  tbl_df() %>%
  rename(
    x_grid = Var1,
    y_grid = Var2
  ) %>%
  mutate(z = coef(model_lm)[1] + coef(model_lm)[2]*x_grid + coef(model_lm)[3]*y_grid) %>%
  .[["z"]] %>%
  matrix(nrow=length(x_grid)) %>%
  t()

# plot points and plane
plot_ly(showscale = FALSE) %>%
  add_markers(
    x = credit$income,
    y = credit$credit_limit,
    z = credit$debt,
    hoverinfo = 'text',
    text = ~paste("x1 - Income: ", credit$income, "</br> x2 - Credit Limit: ", 
                  credit$credit_limit, "</br> y - Debt: ", credit$debt)
  ) %>% 
  layout(
    scene = list(
      xaxis = list(title = "x1 - Income (in $10K)"),
      yaxis = list(title = "x2 - Credit Limit ($)"),
      zaxis = list(title = "y - Debt ($)")
    )
  ) %>% 
  add_surface(
    x = x_grid,
    y = y_grid,
    z = z_grid
  )
```

```{block, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Conduct a new exploratory data analysis with the same outcome variable $y$ being `debt` but with `credit_rating` and `age` as the new explanatory variables $x_1$ and $x_2$. Remember, this involves three things:

a) Looking at the raw values
a) Computing summary statistics of the variables of interest.
a) Creating informative visualizations

What can you say about the relationship between a credit card holder's balance and their credit rating and age?

<!-- CHESTER: I'm not sold on this practice and prefer to assign new variables in R like `Credit_small` instead of overwriting. I seem to remember us agreeing that re-assignment was only OK if we added more variables in Chapter 2-5, not if we chose a subset. We should stay consistent throughout so I'd recommend switching this to a different name as I have with `evals` in Chapters 6 and 7. -->

```{block, type='learncheck', purl=FALSE}
```


### Regression plane {#model3table}

Just as we did when we had a single numerical explanatory variable $x$ in Subsection \@ref(model1table) and when we had a single categorical explanatory variable $x$ in Subsection \@ref(model2table), we fit a regression model and obtained the regression table in our two numerical explanatory variable scenario. To fit a regression model and get a table using `get_regression_table()`, we now use a `+` to consider multiple explanatory variables. In this case since we want to perform a regression of `credit_limit` and `income` simultaneously, we input `debt ~ credit_limit + income`.

```{r, eval=FALSE}
debt_model <- lm(debt ~ credit_limit + income, data = credit)
get_regression_table(debt_model)
```
```{r, echo=FALSE}
debt_model <- lm(debt ~ credit_limit + income, data = credit)
credit_line <- get_regression_table(debt_model) %>%
  pull(estimate)
```
```{r model3-table-output, echo=FALSE}
get_regression_table(debt_model) %>% 
  knitr::kable(
    digits = 3,
    caption = "Multiple regression table", 
    booktabs = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position"))
```

How do we interpret these three values that define the regression plane?

* Intercept: -$385.18 (rounded to two decimal points to represent cents). The intercept in our case represents the credit card balance for an individual who has both a credit `credit_limit` of $0 and `income` of $0. In our data however, the intercept has limited practical interpretation as no individuals had `credit_limit` or `income` values of $0 and furthermore the smallest credit card balance was $0. Rather, it is used to situate the regression plane in 3D space. 
* Credit limit: $0.26. Now that we have multiple variables to consider, we have to add
a caveat to our interpretation: *taking all other variables in our model into account, for every increase of one unit in `credit_limit` (dollars), there is an associated increase of on average $0.26 in credit card balance*. Note:
    + Just as we did in Subsection \@ref(model1table), we are not making any causal statements, only statements relating to the association between credit limit and balance
    + We need to preface our interpretation of the associated effect of `credit_limit` with the statement "taking all other variables into account", in this case `income`, to emphasize that we are now jointly interpreting the associated effect of multiple explanatory variables in the same model and not in isolation.
* Income: -$7.66. Similarly, *taking all other variables into account, for every increase of one unit in `income` (in other words, $1000 in income), there is an associated decrease of on average $7.66 in credit card balance*.

However, recall in Figure \@ref(fig:2numxplot1) that when considered separately, both `credit_limit` and `income` had positive relationships with the outcome variable `debt`. As card holders' credit limits increased their credit card balances tended to increase as well, and a similar relationship held for incomes and balances. In the above multiple regression, however, the slope for `income` is now -7.66, suggesting a *negative relationship* between income and credit card balance. What explains these contradictory results? 

This is known as Simpson's Paradox, a phenomenon in which a trend appears in several different groups of data but disappears or reverses when these groups are combined. We expand on this in Subsection \@ref(simpsonsparadox) where we'll look at the relationship between credit `credit_limit` and credit card balance but split by different income bracket groups.

```{block, type='learncheck', purl=FALSE}
**_Learning check_**
```

**`r paste0("(LC", chap, ".", (lc <- lc + 1), ")")`** Fit a new simple linear regression using `lm(debt ~ credit_rating + age, data = credit)` where `credit_rating` and `age` are the new numerical explanatory variables $x_1$ and $x_2$. Get information about the "best-fitting" line from the regression table by applying the `get_regression_table()` function. How do the regression results match up with the results from your exploratory data analysis above? 

```{block, type='learncheck', purl=FALSE}
```


### Observed/fitted values and residuals {#model3points}

As we did previously in Table \@ref(tab:model3-points-table), let's unpack the output of the `get_regression_points()` function for our model for credit card balance for all `r nrow(credit)` card holders in the dataset. Recall that each card holder corresponds to one of the `r nrow(credit)` rows in the `credit` data frame and also for one of the `r nrow(credit)` 3D points in the 3D scatterplots in Subsection \@ref(model3EDA).

```{r, eval=FALSE}
regression_points <- get_regression_points(debt_model)
regression_points
```
```{r model3-points-table, echo=FALSE}
set.seed(76)
regression_points <- get_regression_points(debt_model)
regression_points %>%
  slice(1:5) %>%
  knitr::kable(
    digits = 3,
    caption = "Regression points (first 5 rows of 400)",
    booktabs = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position"))
```

Recall the format of the output:

* `debt` corresponds to $y$ (the observed value)
* `debt_hat` corresponds to $\widehat{y}$ (the fitted value)
* `residual` corresponds to $y - \widehat{y}$ (the residual)



***



## Related topics

### Interaction vs parallel slopes model {#model-selection}

How do you choose between parallel slopes and interaction models?

```{r, eval = FALSE}
# Interaction model
ggplot(MA_schools, aes(x = perc_disadvan, y = average_sat_math, color = size))+
  geom_point(alpha = 0.25) +
  geom_smooth(method = "lm", se = FALSE ) +
  labs(x = "Percent economically disadvantaged", y = "Math SAT Score", 
       color = "School size", title = "Interaction model")

# Parallel slopes model
gg_parallel_slopes(y = "average_sat_math", num_x = "perc_disadvan", 
                   cat_x = "size", data = MA_schools, alpha = 0.25) + 
  labs(x = "Percent economically disadvantaged", y = "Math SAT Score", 
       color = "School size", title = "Parallel slopes model") 
```

Figure \@ref(fig:numxcatx-comparison-2)

```{r numxcatx-comparison-2, fig.width = 8, echo = FALSE, warning=FALSE, fig.cap="Comparison of interaction and parallel slopes models."}
p1 <- ggplot(MA_schools, aes(x = perc_disadvan, y = average_sat_math, color = size))+
  geom_point(alpha = 0.25) +
  geom_smooth(method = "lm", se = FALSE ) +
  labs(x = "Percent economically disadvantaged", y = "Math SAT Score", 
       color = "School size", title = "Interaction model") + 
  theme(legend.position = "none")
p2 <- gg_parallel_slopes(y = "average_sat_math", num_x = "perc_disadvan", 
                         cat_x = "size", data = MA_schools, alpha = 0.25) + 
  labs(x = "Percent economically disadvantaged", y = "Math SAT Score", 
       color = "School size", title = "Parallel slopes model")  +
  theme(axis.title.y = element_blank())
p1 + p2
```

```{r, eval = FALSE}
model_2_interaction <- lm(average_sat_math ~ perc_disadvan * size, data = MA_schools)
get_regression_table(model_2_interaction)
```
```{r model2-interaction, echo=FALSE}
model_2_interaction <- lm(average_sat_math ~ perc_disadvan * size, data = MA_schools)
get_regression_table(model_2_interaction) %>% 
  knitr::kable(
    digits = 3,
    caption = "Interaction model regression table", 
    booktabs = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position"))
```

```{r, eval = FALSE}
model_2_parralel_slopes <- lm(average_sat_math ~ perc_disadvan + size, data = MA_schools)
get_regression_table(model_2_parralel_slopes)
```
```{r model2-parallel-slopes, echo=FALSE}
model_2_parralel_slopes <- lm(average_sat_math ~ perc_disadvan + size, data = MA_schools)
get_regression_table(model_2_parralel_slopes) %>% 
  knitr::kable(
    digits = 3,
    caption = "Parallel slopes regression table", 
    booktabs = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position"))
```



### Correlation coefficient {#correlationcoefficient2}

Recall from Table \@ref(tab:model3-correlation) that we saw the correlation
coefficient between `income` in thousands of dollars and credit card `debt`
was 0.464. What if in instead we looked at the correlation coefficient between
`income` and credit card `debt`, but where `income` was in dollars and not
thousands of dollars? This can be done by multiplying `income` by 1000.

```{r, eval=FALSE}
library(ISLR)
credit %>% 
  select(debt, income) %>% 
  mutate(income = income * 1000) %>% 
  cor()
```
```{r cor-credit-2, echo=FALSE}
library(ISLR)
credit %>% 
  select(debt, income) %>% 
  mutate(income = income * 1000) %>% 
  cor() %>% 
  knitr::kable(
    digits = 3,
    caption = "Correlation between income (in dollars) and credit card balance", 
    booktabs = TRUE
  ) %>% 
  kable_styling(font_size = ifelse(knitr:::is_latex_output(), 10, 16),
                latex_options = c("HOLD_position"))
```

We see it is the same! We say that the correlation coefficient is invariant to linear
transformations! In other words,

* the correlation between $x$ and $y$ will be the same as
* the correlation between $a\times x + b$ and $y$ where $a$ and $b$ are numerical values (real numbers in mathematical terms).


### Simpson's Paradox {#simpsonsparadox}

Recall in Section \@ref(model3), we saw the two following seemingly contradictory results when studying the relationship between credit card balance, credit limit, and income. On the one hand, the right hand plot of Figure \@ref(fig:2numxplot1) suggested that credit card balance and income were positively related:

```{r echo=FALSE, fig.height=4, fig.cap="Relationship between credit card balance and credit limit/income"}
model3_balance_vs_limit_plot + model3_balance_vs_income_plot
```

On the other hand, the multiple regression in Table \@ref(tab:model3-table-output), suggested that when modeling credit card balance as a function of both credit limit and income at the same time, credit limit has a negative relationship with balance, as evidenced by the slope of -7.66. How can this be?

First, let's dive a little deeper into the explanatory variable `credit_limit`. Figure \@ref(fig:credit-limit-quartiles) shows a histogram of all `r nrow(credit)` values of `credit_limit`, along with vertical red lines that cut up the data into quartiles, meaning: 

1. 25% of credit limits were between \$0 and \$3088. Let's call this the "low" credit limit bracket.
1. 25% of credit limits were between \$3088 and \$4622. Let's call this the "medium-low" credit limit bracket.
1. 25% of credit limits were between \$4622 and \$5873. Let's call this the "medium-high" credit limit bracket.
1. 25% of credit limits were over \$5873. Let's call this the "high" credit limit bracket.

```{r credit-limit-quartiles, echo=FALSE, fig.height=4, fig.cap="Histogram of credit limits and quartiles"}
ggplot(credit, aes(x = credit_limit)) +
  geom_histogram(color = "white") +
  geom_vline(xintercept = quantile(credit$credit_limit, probs = c(0.25, 0.5, 0.75)), col = "red", linetype = "dashed")
```

Let's now display

1. The scatterplot showing the relationship between credit card balance and limit (the right-hand plot of Figure \@ref(fig:2numxplot1)).
1. The scatterplot showing the relationship between credit card balance and limit now with a color aesthetic added corresponding to the credit limit bracket.

```{r, 2numxplot4, fig.height=4, echo=FALSE, fig.cap="Relationship between credit card balance and income for different credit limit brackets"}
credit <- credit %>% 
  mutate(limit_bracket = cut_number(credit_limit, 4)) %>% 
  mutate(limit_bracket = fct_recode(limit_bracket,
    "low" =  "[855,3.09e+03]",
    "medium-low" = "(3.09e+03,4.62e+03]", 
    "medium-high" = "(4.62e+03,5.87e+03]", 
    "high" = "(5.87e+03,1.39e+04]"
  ))

model3_balance_vs_income_plot <- ggplot(credit, aes(x = income, y = debt)) +
  geom_point() +
  labs(x = "Income (in $1000)", y = "Credit card balance (in $)", 
       title = "Debt vs income\n(overall)") +
  geom_smooth(method = "lm", se = FALSE) +
  scale_y_continuous(limits = c(0, NA))

model3_balance_vs_income_plot_colored <- ggplot(credit, aes(x = income, y = debt, col = limit_bracket)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Income (in $1000)", y = "Credit card balance (in $)", 
       color = "Credit limit\nbracket", title = "Debt vs income\n(colored by credit limit bracket)") + 
  theme(legend.position = "none") +
  scale_y_continuous(limits = c(0, NA))
  
model3_balance_vs_income_plot + model3_balance_vs_income_plot_colored
```

In the right-hand plot, the

* Red points (bottom-left) correspond to the low credit limit bracket.
* Green points correspond to the medium-low credit limit bracket.
* Blue points correspond to the medium-high credit limit bracket.
* Purple points (top-right) correspond to the high credit limit bracket.

The left-hand plot focuses of the relationship between balance and income in aggregate, but the right-hand plot focuses on the relationship between balance and income *broken down by credit limit bracket*. Whereas in aggregate there is an overall positive relationship, when broken down we now see that for the low (red points), medium-low (green points), and medium-high (blue points) income bracket groups, the strong positive relationship between credit card balance and income disappears! Only for the high bracket does the relationship stay somewhat positive. In this example, credit limit is a *confounding variable* for credit card balance and income.

<!--
Alternatively, we could also have used facets, where each facet has roughly 25% of people based
on the credit limit bracket. However, IMO the above plot is easier to read.

```{r, 2numxplot5, echo=FALSE, warning=FALSE, fig.cap="Relationship between credit card balance and income for different credit limit brackets"}
ggplot(credit, aes(x = income, y = debt)) +
  geom_point() +
  facet_wrap(~limit_bracket) +
  geom_smooth(method = "lm", se = FALSE) +
  labs(x = "Income (in $1000)", y = "Credit card balance (in $)")
```
--> 



***



## Conclusion

### Additional resources

An R script file of all R code used in this chapter is available [here](scripts/07-multiple-regression.R).

### What's to come?

Congratulations! We're ready to proceed to the third portion of this book: "statistical inference" using a new package called `infer`.  Once we've covered Chapters \@ref(sampling) on sampling, \@ref(confidence-intervals) on confidence intervals, and \@ref(hypothesis-testing) on hypothesis testing, we'll come back to the models we've seen in "data modeling" in Chapter \@ref(inference-for-regression) on inference for regression. As we said at the end of Chapter \@ref(regression), we'll see why we've been conducting the residual analyses from Subsections \@ref(model3residuals) and \@ref(model4residuals). We are actually verifying some very important assumptions that must be met for the `std_error` (standard error), `p_value`, `conf_low` and `conf_high` (the end-points of the confidence intervals) columns in our regression tables to have valid interpretation. Up next:

<center><img src="images/flowcharts/flowchart/flowchart.006.png" title="ModernDive flowchart" width="800"/></center>




